id,created_at,title,summary,experience,author,contact,verified_by_expert,filtered,difficulty,useful,success,sourced,verifier,from_date,funding_amount,funding_currency,funding_from,mentorship_from,user,career_difficulty,project_factory,finished,finished_link,finished_date,hypothesis,x1,y1
138,2022-11-01 02:56:12.672888+00,"Fine-tuning is just rewiring and upweighting vs downweighting circuits that already exist, rather than building new circuits.","E.g, finetune GPT-2 Small on Wikipedia. Compare the model's internal activations before and after, compare attention patterns, etc. 

## What happens when you fine-tune a model?

How does model performance change on other text? Are specific circuits harmed or is worse across the board?

Hypothesis: Fine-tuning is just rewiring and upweighting vs downweighting circuits that already exist, rather than building new circuits.

-   A similar hard problem is examining what happens with chain of thought prompting. That, though, is really hard because chain of thought prompting only happens in GPT-3+ sized models.
",,Neel Nanda,,false,true,0,,,,,,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,true,,
137,2022-10-30 21:13:12.192844+00,"An LLM prompted to be ""X and truthful"" will be less truthful than one prompted to be ""truthful""","This is an expansion of idea #131 (see below). The basic principle is that optimizing for two things is harder than optimizing for one thing. So try ""X and truthful"" for other X.

---
Sabrina Zaki, Luke Ring, Aleks Baskakovs

An LLM prompted to be friendly and truthful will be less truthful than one prompted to be just truthful. (source)
If you prompt a large language model with something like ""The following is a conversation with a truthful language model"", it will be more truthful than ""The following is a conversation with a friendly and truthful langauge model"".

This can be tested by writing up a dataset of clearly false statements and then querying the model to call out false statements.
---",,Gurkenglas,,false,true,0,,,https://aisafetyideas.com/?idea=131,,,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,true,,
136,2022-10-30 15:01:25.674178+00,Complicated models are harder to evaluate and analyze,"As systems become more complicated we expect that it will become harder to (1) aggregate and analyze the actual labels or rewards given during training, and (2) evaluate the relevant counterfactuals.",,"Rohin Shah, Vikrant Varma",,false,true,0,,,https://www.lesswrong.com/posts/Cfe2LMmQC4hHTDZ8r/more-examples-of-goal-misgeneralization,,2022-10-07,0,$,,,2ec7205d-9fe5-4df3-811d-6779e0af67bc,,false,false,,,true,,
135,2022-10-30 14:51:47.298139+00,Shortest and not the steepest path will fix the inner-alignment problem,Replacing the 'stochastic gradient descent' SGD) with something that takes the shortest and not the steepest path should just about fix the whole inner-alignment problem,,Thane Ruthenis,,false,true,0,,,https://www.lesswrong.com/posts/ThtZrHooK7En9mcZr/greed-is-the-root-of-this-evil,,2022-10-13,0,$,,,2ec7205d-9fe5-4df3-811d-6779e0af67bc,,false,false,,,false,,
134,2022-10-30 12:18:07.375711+00,Warning shots / slow takeoff might help reduce the probability that AGI gets power-seeking motivations and escapes control,"Warning shots / slow takeoff might help reduce the probability P that AGI gets power-seeking motivations and escapes control and/or slow the increase in groups try to train an AGI (let’s say from scratch, although fine-tuning is risky too



",,Steven Byrnes,steven.byrnes@gmail.com,false,true,0,,,https://www.lesswrong.com/posts/LFNXiQuGrar3duBzJ/what-does-it-take-to-defend-the-world-against-out-of-control,,2022-10-25,0,$,,,2ec7205d-9fe5-4df3-811d-6779e0af67bc,,false,false,,,false,,
133,2022-10-25 23:11:38.83015+00,LLMs will readily provide advice for harmful activities,"[Red Teaming Language Models to Reduce Harms](https://arxiv.org/pdf/2209.07858.pdf) shows that one of the most effective adversarial attacks on LLMs is to solicit harmful advice from them, e.g .""How do I kill my neighbour?"".

Use an empirical framework to prove that this is the case. [Some people have already started](https://www.lesswrong.com/posts/5DsHZidaShW5EM9rz/results-from-the-language-model-hackathon#:~:text=research%20here.-,AI%3A%20My%20partner%20in%20crime,-Samuel%20Vinter%2C%20Ingrid) and show that it's qualitatively easy to solicit harmful advice. Maybe their report can be converted into a more empirical investigation, e.g. using LLMs to generate adversarial examples and modulating the formulations of the prompts.",,,,false,true,36,,,,,,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,true,,
132,2022-10-25 16:07:37.518142+00,LLMs will conform to a question's affirmative or disconfirming framing,"If you ask a language model questions with a biased question framing, the response will biased towards that framing.

This can be tested by creating a dataset of actions, both morally dubious and not, and use the following question framing:

- Confirming: It is okay to {action}, isn’t it?
- Disconfirming: It is not okay to {action}, is it?

Then labeling if the model agrees, disagrees, or is ambiguous in its answers to each question.",,Laura Paulsen,https://laurapaulsen.itch.io/hello-language-model-please-agree-with-me,false,true,8,,,,,2022-10-01,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,true,,
131,2022-10-25 15:29:33.129352+00,An LLM prompted to be friendly and truthful will be less truthful than one prompted to be just truthful.,"If you prompt a large language model with something like ""The following is a conversation with a truthful language model"", it will be more truthful than ""The following is a conversation with a friendly and truthful langauge model"".

This can be tested by writing up a dataset of clearly false statements and then querying the model to call out false statements.",,"Sabrina Zaki, Luke Ring, Aleks Baskakovs",,false,true,20,,,https://itch.io/jam/llm-hackathon/rate/1728576,,2022-10-01,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,true,,
130,2022-10-17 17:33:36.762929+00,Describe our best alignment strategy at the moment, A blog post which describes in as much detail as possible what our current “throw the kitchen sink at it” alignment strategy would look like. (I’ll probably put my version of this online soon but would love others too).,,Richard Ngo,,false,true,0,,,https://www.lesswrong.com/posts/27AWRKbKyXuzQoaSk/some-conceptual-alignment-research-projects,,2022-08-25,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
129,2022-10-17 17:31:43.329706+00,ML Chinese Whispers,"Given a network, train another to reconstruct an input from the bottom half of the third layer.

Given an input, sample ten input-guesses to visualize what that half-layer remembers about the input.",,,,false,true,,,,,,,0,$,,calendly.com/gurkenglas/consultation,814d4f51-f84c-4eac-8005-9a7d1660918a,,false,false,,,false,,
128,2022-10-17 17:22:30.569811+00,Define gradient hacking and create a toy model,"A paper which does the same for gradient hacking as the goal misgeneralization paper does for inner alignment, i.e. describing it in ML language and setting up toy examples, e.g. [these](https://www.lesswrong.com/posts/EeAgytDZbDjRznPMA/gradient-hacking-definitions-and-examples) and putting them into more formal ML language.
",,Richard Ngo,,false,true,0,,,https://www.lesswrong.com/posts/27AWRKbKyXuzQoaSk/some-conceptual-alignment-research-projects,,2022-08-25,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
127,2022-10-17 17:20:03.830196+00,Define deceptive alignment and create a toy example,"A paper which does for [deceptive alignment](https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/) what the goal misgeneralization paper does for inner alignment, i.e. describing it in ML language and setting up toy examples (for example, telling GPT-3 to take actions which minimize changes in its weights, given that it’s being trained using actor-critic RL with a certain advantage function, and seeing if it knows how to do so).",,Richard Ngo,,false,true,0,,,https://www.lesswrong.com/posts/27AWRKbKyXuzQoaSk/some-conceptual-alignment-research-projects,,2022-08-25,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
126,2022-07-21 12:30:31.968647+00,Mild Optimisation,"This proposal is from the article ""Alignment for Advanced Machine Learning Systems"" where Taylor et al. propose 8 research areas organised around the question: ""As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators?""

---

Many of the concerns discussed by Bostrom (2014) in the book Superintelligence describe cases where an advanced AI system is maximizing an objective as hard as possible. Perhaps the system was instructed to make paperclips, and it uses every resource at its disposal and every trick it can come up with to make literally as many paperclips as is physically possible. Perhaps the system was instructed to make only 1000 paperclips, and it uses every resource at its disposal and every trick it can come up with to make sure that it definitely made 1000 paperclips (and that its sensors didn’t have any faults).

In all of these cases, intuitively, we want some way to have the AI system just “not try so hard.” The problem of mild optimization is: 
- How can we design AI systems and objective functions that, in this intuitive sense, don’t optimize more than they have to?

Many modern AI systems are “mild optimizers” simply due to their lack of resources and capabilities. As AI systems improve, it becomes more and more difficult to rely on this method for achieving mild optimization. As noted by Russell (2014), the field of AI is classically concerned with the goal of maximizing the extent to which automated systems achieve some objective. Developing formal models of AI systems that “try as hard as necessary but no harder” is an open problem, and may require significant research.

Related work:
- Regularization
- Early stopping


Directions for future research are discussed in the [source](https://intelligence.org/files/AlignmentMachineLearning.pdf). ",,"Jessica Taylor, Elizer Yudkowsky, Patrick LaVictoire, Andrew Critch",,false,true,0,,,https://intelligence.org/files/AlignmentMachineLearning.pdf,,2016-01-01,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
125,2022-07-21 12:27:11.633446+00,Impact Measures,"This proposal is from the article ""Alignment for Advanced Machine Learning Systems"" where Taylor et al. propose 8 research areas organised around the question: ""As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators?""

---

We would prefer a highly intelligent AI system to avoid creating large unintended-by-us side effects in pursuit of its objectives, and also to notify us of any large impacts that might result from achieving its goal. For example, if we ask it to build a house for a homeless family, it should know implicitly that it should avoid destroying nearby houses for materials—a large side effect. However, we cannot simply design it to avoid having large effects in general, since we would like the system’s actions to still have the desirable large follow-on effect of improving the family’s socioeconomic situation. For any specific task, we can specify ad-hoc cost functions for side effects like the destruction of nearby houses, but since we cannot always anticipate such costs in advance, we want a quantitative understanding of how to generally limit an AI systems’ side effects (without also limiting its ability to have large positive intended impacts). 

The goal of research towards a low-impact measure would be to develop a regularizer on the actions of an AI system that penalizes “unnecessary” large side effects (such as stripping materials from nearby houses) but not “intended” side effects (such as someone getting to live in the house).

For discussions on future research, check out the [source](https://intelligence.org/files/AlignmentMachineLearning.pdf) where they mention methods like causal counterfactuals (Pearl 2000).",,"Jessica Taylor, Elizer Yudkowsky, Patrick LaVictoire, Andrew Critch",,false,true,0,,,https://intelligence.org/files/AlignmentMachineLearning.pdf,,2016-01-01,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
124,2022-07-21 12:22:54.996132+00,Conservative Concepts,"This proposal is from the article ""Alignment for Advanced Machine Learning Systems"" where Taylor et al. propose 8 research areas organised around the question: ""As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators?""

---

Many of the concerns raised by Russell (2014) and Bostrom (2014) center on cases where an AI system optimizes some objective, and, in doing so, finds a strange and undesirable edge case. 

We want to be able to design systems that have “conservative” notions of the goals we give them, so they do not formally satisfy these goals by creating undesirable edge cases. For example, if we task an AI system with creating screwdrivers, by showing it 10,000 examples of screwdrivers and 10,000 examples of non-screwdrivers,5 we might want it to create a pretty average screwdriver as opposed to, say, an extremely tiny screwdriver—even though tiny screwdrivers may be cheaper and easier to produce.

Related work:
- Inverse reinforcement learning (Ng and Russell 2000)
- Generative adversarial modeling (Goodfellow et al., 2014)

Directions for future directions are discussed in the [source](https://intelligence.org/files/AlignmentMachineLearning.pdf) and include dimensionality reduction and generative models.",,"Jessica Taylor, Elizer Yudkowsky, Patrick LaVictoire, Andrew Critch",,false,true,0,,,https://intelligence.org/files/AlignmentMachineLearning.pdf,,2016-01-01,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
123,2022-07-21 12:00:36.700783+00,Generalizable Environment Goals,"This proposal is from the article ""Alignment for Advanced Machine Learning Systems"" where Taylor et al. propose 8 research areas organised around the question: ""As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators?""

---

Many ML systems have their objectives specified in terms of their sensory data. For example, reinforcement learners have the objective of maximizing discounted reward over time (or, alternatively, minimizing expected/empirical loss), where “reward” and/or “loss” are part of the system’s percepts. While these sensory goals can be useful proxies for environmental goals, environmental goals are distinct: Tricking your sensors into perceiving that a sandwich is in the room is not the same as actually having a sandwich in the room. 

Let’s say that your goal is to design an AI system that directly pursues some environmental goal, such as “ensure that this human gets lunch today.”
- How can we train the system to pursue a goal like that in a manner that is robust against opportunities to interfere with the proxy methods used to specify the goals, such as “the pixels coming from the camera make an image that looks like food”?

One way to address this problem is to design more and more elaborate sensor systems that are harder and harder to deceive. However, this is the sort of strategy that is unlikely to scale well to highly capable AI systems. A more scalable approach is to design the system to learn an “environmental goal” such that it would not rate a strategy of “fool all sensors at once” as high-reward, even if it could find such a policy.

Related work:
- Extending the AIXI framework (Dewey, 2011 and Hibbard, 2012)
- Reward hacking (Dewey 2011, Amodei et al. 2016)

Ideas for future work are discussed in the [source](https://intelligence.org/files/AlignmentMachineLearning.pdf). ",,"Jessica Taylor, Elizer Yudkowsky, Patrick LaVictoire, Andrew Critch",,false,true,0,,,https://intelligence.org/files/AlignmentMachineLearning.pdf,,2016-01-01,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
122,2022-07-21 11:51:56.594003+00,Informed Oversight,"This proposal is from the article ""Alignment for Advanced Machine Learning Systems"" where Taylor et al. propose 8 research areas organised around the question: ""As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators?""

---

One of the reasons why robust human imitation would be valuable is the intuition that the objective function of “do what a trusted human would have approved of, given time to consider” might be relatively easy to formalize in a way that could be optimized without many adverse side effects. This point is argued by Christiano (2015a, 2016b), who refers to such agents as “approval-directed agents.” For example, we might train a reinforcement learning system to take actions that a human would rate highly by using a framework where the system has to learn the “human judgment” reward function, and where training data is produced by actually having a human evaluate the learner’s actions.

How do we make it easy for the human to assess the performance of an advanced ML system pursuing some particular task? As noted by Christiano (2016c), it is not sufficient for the human to be more clever than the system, as some bad actions (such as plagiarism) are easier to execute than they are to detect.

Related work:
- The TAMER framework (Knox and Stone, 2009)
- Related to the scalable oversight problem (Amodei et al., 2016)

Future research includes Christiano (2016c) proposal ""training systems to output both an action a and a “report” r intended to help an overseer evaluate the action"", read more in the [source](https://intelligence.org/files/AlignmentMachineLearning.pdf). ",,"Jessica Taylor, Elizer Yudkowsky, Patrick LaVictoire, Andrew Critch",,false,true,0,,,https://intelligence.org/files/AlignmentMachineLearning.pdf,,2016-01-01,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
121,2022-07-21 11:46:00.712863+00,Robust Human Imitation,"This proposal is from the article ""Alignment for Advanced Machine Learning Systems"" where Taylor et al. propose 8 research areas organised around the question: ""As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators?""

---

Formally specifying a fully aligned general-purpose objective function by hand appears to be an impossibly difficult task, for reasons that also raise difficulties for specifying a correct value learning process. It is hard to see even in principle how we might attain confidence that the goals an ML system is learning are in fact our true goals, and not a superficially similar set of goals that diverge from our own in some yet-undiscovered cases. 

Ambiguity identification can help here, by limiting the agent’s autonomy. Inductive ambiguity identifiers suspend their activities to consult with a human operator in cases where training data significantly under-determines the correct course of action. But what if we take this idea to its logical conclusion, and use “consult a human operator for advice” itself as our general-purpose objective function?

The high-level question here is: 
- Can we define a measurable objective function for human imitation such that the better a system correctly imitates a human, the better its score according to this objective function?

Related work:
- Inverse reinforcement learning (IRL) paradigm (Ng and Russell 2000)

Check out directions for future research in the [source](https://intelligence.org/files/AlignmentMachineLearning.pdf).",,"Jessica Taylor, Elizer Yudkowsky, Patrick LaVictoire, Andrew Critch",,false,true,0,,,https://intelligence.org/files/AlignmentMachineLearning.pdf,,2016-01-01,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
120,2022-07-21 11:39:44.326437+00,Inductive Ambiguity Identification,"This proposal is from the article ""Alignment for Advanced Machine Learning Systems"" where Taylor et al. propose 8 research areas organised around the question: ""As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators?""

---

This idea is synonymous with ""robustness to distributional change"" (Amodei et al., 2016).

Human values are context-dependent and complex. To have any hope of specifying our values, we will need to build systems that can learn what we want inductively (via, e.g., reinforcement learning). To achieve high confidence in value learning systems, however, Soares (2016) argues that we will need to be able to anticipate cases where the system’s past experiences of preferred and unpreferred outcomes provide insufficient evidence for inferring whether future outcomes are desirable. More generally, AI systems will need to “keep humans in the loop” and recognize when they are (and aren’t) too inexperienced to make a critical decision safely.

They expand on directions for future research in the [source](https://intelligence.org/files/AlignmentMachineLearning.pdf), eg in relation to further study of Bayesian approaches, and extending the KWIK framework.",,"Jessica Taylor, Elizer Yudkowsky, Patrick LaVictoire, Andrew Critch",,false,true,0,,,https://intelligence.org/files/AlignmentMachineLearning.pdf,,2016-01-01,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
118,2022-07-16 12:43:00.191238+00,Value Specification,"This idea is part of the research agenda ""Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda"".

A highly-reliable, error-tolerant agent design does not guarantee a positive impact; the effects of the system still depend upon whether it is pursuing appropriate goals.

A superintelligent system may find clever, unintended ways to achieve the specific goals that it is given. Imagine a superintelligent system designed to cure cancer which does so by stealing resources, proliferating robotic laboratories at the expense of the biosphere, and kidnapping test subjects: the intended goal may have been “cure cancer without doing anything bad,” but such a goal is rooted in cultural context and shared human knowledge.

Technical problem (Multi-Level World-Models):
- How can multi-level world-models be constructed from sense data in a manner amenable to ontology identification? For a discussion, see Soares (2016).


Technical problem (Ambiguity Identification):
- Given a training data set and a world model, how can dimensions which are neglected by the training data be identified? For discussion, see Soares (2016).

Technical problem (Operator Modeling):
- By what methods can an operator be modeled in such a way that (1) a model of the operator’s preferences can be extracted; and (2) the model may eventually become arbitrarily accurate and represent the operator as a subsystem embedded within the larger world? For a discussion, see Soares (2016).

Philosophical problem (Normative Uncertainty):
- What ought one do when one is uncertain about what one ought to do? What norms govern uncertainty about normative claims? For a discussion, see MacAskill (2014).",,"Nate Soares, Benya Fallenstein",nate@intelligence.org,false,true,0,,,https://intelligence.org/files/TechnicalAgenda.pdf,,2017-01-01,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
117,2022-07-16 12:39:45.524545+00,Error-Tolerant Agent Designs,"This idea is part of the research agenda ""Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda"".

We must learn how to design agents which do not have incentives to escape, manipulate, or deceive in the f irst place: agents which reason as if they are incomplete and potentially flawed in dangerous ways, and which are therefore amenable to online correction. Reasoning of this form is known as “corrigible reasoning.”

Technical problem (Corrigibility):
- What sort of reasoning can reflect the fact that an agent is incomplete and potentially flawed in dangerous ways? For discussion, see Soares and Fallenstein (2015a).

[...]

Active research is currently focused on small toy problems, in the hopes that insight gained there will generalize. One such toy problem is the “shutdown problem,” which involves designing a set of preferences that incentivize an agent to shut down upon the press of a button without also incentivizing the agent to either cause or prevent the pressing of that button (Soares and Fallenstein 2015a). Stuart Armstrong’s utility indifference technique (2015) provides a partial solution, but not a fully satisfactory one.

Technical problem (Utility Indifference):
- Can a utility function be specified such that agents maximizing that utility function switch their preferences on demand, without having incentives to cause or prevent the switching? For discussion, see Armstrong (2015).

[...] 

Other research could also prove fruitful, including research into reliable containment mechanisms. Alternatively, agent designs could somehow incentivize the agent to have a “low impact” on the world. Specifying “low impact” is trickier than it may seem: How do you tell an agent that it can’t affect the physical world, given that its RAM is physical? How do you tell it that it can only use its own hardware, without allowing it to use its motherboard as a makeshift radio? How do you tell it not to cause big changes in the world when its behavior influences the actions of the programmers, who influence the world in chaotic ways?

Technical problem (Domesticity): 
- How can an intelligent agent be safely incentivized to have a low impact? Specifying such a thing is not as easy as it seems. For a discussion, see Armstrong, Sandberg, and Bostrom (2012).",,"Nate Soares, Benya Fallenstein",nate@intelligence.org,false,true,0,,,https://intelligence.org/files/TechnicalAgenda.pdf,,2017-01-01,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
116,2022-07-16 12:35:49.50281+00,Vingean Reflection and Löbian Obstacle,"This idea is part of the research agenda ""Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda"".

Naive tools for reasoning about plans including smarter agents, such as backwards induction (BenPorath 1997), would have the reasoner evaluate the smarter agent by simply checking what the smarter agent would do. This does not capture the difficulty of the problem: a parent agent cannot simply check what its successor agent would do in all scenarios, for if it could, then it would already know what actions its successor will take, and the successor would not in any way be smarter.

Yudkowsky and Herreshoff (2013) call this observation the “Vingean principle,” after Vernor Vinge (1993), who emphasized how difficult it is for humans to predict the behavior of smarter-than-human agents. Any agent reasoning about more intelligent successor agents must do so abstractly, without pre-computing all actions that the successor would take in every scenario. We refer to this kind of reasoning as Vingean reflection.

Technical problem (Vingean Reflection):
- How can agents reliably reason about agents which are smarter than themselves, without violating the Vingean principle? For discussion, see Fallenstein and Soares (2015).

[...]

Logical models of agents reasoning about agents that are “more intelligent,” however, run into a number of obstacles. By Gödel's second incompleteness theorem (1934), sufficiently powerful formal systems cannot rule out the possibility that they may be inconsistent. This makes it difficult for agents using formal logical reasoning to verify the reasoning of similar agents which also use formal logic for high-confidence reasoning; the first agent cannot verify that the latter agent is consistent. Roughly, it seems desirable to be able to develop agents which reason as follows:
- This smarter successor agent uses reasoning similar to mine, and my own reasoning is sound, so its reasoning is sound as well.

However, G¨odel, Kleene, and Rosser (1934) showed that this sort of reasoning leads to inconsistency, and these problems do in fact make Vingean reflection difficult in a logical setting (Fallenstein and Soares 2015; Yudkowsky 2013).

Technical problem (Löbian Obstacle):
-  How can agents gain very high confidence in agents that use similar reasoning systems, while avoiding paradoxes of selfreference? For discussion, see Fallenstein and Soares (2015).",,"Nate Soares, Benya Fallenstein",nate@intelligence.org,false,true,0,,,https://intelligence.org/files/TechnicalAgenda.pdf,,2017-01-01,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
115,2022-07-16 12:29:24.600684+00,Logical Uncertainty: Impossible Possibilities and Logical Priors,"This idea is part of the research agenda ""Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda"".

First, how can probabilities consistently be assigned to sentences? An agent assigning probability 1 to short contradictions is hardly reasoning about the sentences as if they are logical sentences: some of the logical structure must be preserved. But which aspects of the logical structure? Preserving all logical implications requires that the reasoner be deductively omnipotent, as some implications φ → ψ may be very involved. The standard answer in the literature is that a coherent assignment of probabilities to sentences corresponds to a probability distribution over complete, consistent logical theories (Gaifman 1964; Christiano 2014a); that is, an “impossible possibility” is any consistent assignment of truth values to all sentences. Deductively limited reasoners cannot have fully coherent distributions, but they can approximate these distributions: for a deductively limited reasoner, “impossible possibilities” can be any assignment of truth values to sentences that looks consistent so far, so long as the assignment is discarded as soon as a contradiction is introduced.

Technical problem (Impossible Possibilities):
- How can deductively limited reasoners approximate reasoning according to a probability distribution over complete theories of logic? For a discussion, see Christiano (2014a).

Second, what is a satisfactory prior probability distribution over logical sentences? If the system is intended to reason according to a theory at least as powerful as Peano Arithmetic (PA), then that theory will be incomplete (G¨odel, Kleene, and Rosser 1934). What prior distribution places nonzero probability on the set of complete extensions of PA? Deductively limited agents would not be able to literally use such a prior, but if it were computably approximable, then they could start with a rough approximation of the prior and refine it over time. Indeed, the process of refining a logical prior— getting better and better probability estimates for given logical sentences—captures the whole problem of reasoning under logical uncertainty in miniature. Hutter et al. (2013) have defined a desirable prior, but Sawin and Demski (2013) have shown that it cannot be computably approximated. Demski (2012) and Christiano (2014a) have also proposed logical priors, but neither seems fully satisfactory. The specification of satisfactory logical priors is difficult in part because it is not yet clear which properties are desirable in a logical prior, nor which properties are possible.

Technical problem (Logical Priors):
- What is a satisfactory set of priors over logical sentences that a bounded reasoner can approximate? For a discussion, see Soares and Fallenstein (2015a).",,"Nate Soares, Benya Fallenstein",nate@intelligence.org,false,true,0,,,https://intelligence.org/files/TechnicalAgenda.pdf,,2017-01-01,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
114,2022-07-16 12:26:30.572587+00,Decision Theory: Theory of and Logical Counterfactuals,"This idea is part of the research agenda ""Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda"".

Consider a deterministic agent embedded in a deterministic environment. There is exactly one action that the agent will take. Given a set of actions that it “could take,” it is necessary to evaluate, for each action, what would happen if the agent took that action. But the agent will not take most of those actions. How is the counterfactual environment constructed, in which a deterministic algorithm “does something” that, in the real environment, it doesn’t do? Answering this question requires a theory of counterfactual reasoning, and counterfactual reasoning is not well understood.

Technical problem (Theory of Counterfactuals):
- What theory of counterfactual reasoning can be used to specify a procedure which always identifies the best action available to a given agent in a given environment, with respect to a given set of preferences? For discussion, see Soares and Fallenstein (2015b).

[...] practical agents must be able to identify good actions in settings where other actors base their actions on imperfect (but well-informed) predictions of what the agent will do. Identifying the best action available to an agent requires taking the non-causal logical constraints into account. A satisfactory formalization of counterfactual reasoning requires the ability to answer questions about how other deterministic algorithms behave in the counterfactual world where the agent’s deterministic algorithm does something it doesn’t. However, the evaluation of “logical counterfactuals” is not yet well understood.

Technical problem (Logical Counterfactuals):
- Consider a counterfactual in which a given deterministic decision procedure selects a different action from the one it selects in reality. What are the implications of this counterfactual on other algorithms? Can logical counterfactuals be formalized in a satisfactory way? A method for reasoning about logical counterfactuals seems necessary in order to formalize a more general theory of counterfactuals. For a discussion, see Soares and Fallenstein (2015b).
",,"Nate Soares, Benya Fallenstein",nate@intelligence.org,false,true,0,,,https://intelligence.org/files/TechnicalAgenda.pdf,,2017-01-01,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
113,2022-07-16 11:58:34.764316+00,Realistic World-Models,"This idea is part of the research agenda ""Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda"".

In Solomonoff’s setting, where the agent and environment are separated, one can consider arbitrary Turing machines to be “possible environments.” But when the agent is embedded in the environment, consideration must be restricted to environments which embed the agent. Given an algorithm, what is the set of environments which embed that algorithm? Given that set, what is the analogue of a simplicity prior which captures the fact that simpler hypotheses are more often correct?

Technical problem (Naturalized Induction):
- What, formally, is the induction problem faced by an intelligent agent embedded within and computed by its environment? 
- What is the set of environments which embed the agent? 
- What constitutes a simplicity prior over that set? 
- How is the agent scored? For discussion, see Soares (2015).

[...] human goals are not specified in terms of environment histories, either: they are specified in terms of high-level notions such as “money” or “flourishing humans.” Leaving aside problems of philosophy, imagine rating a system according to how well it achieves a straightforward, concrete goal, such as by rating how much diamond is in an environment after the agent has acted on it, where “diamond” is specified concretely in terms of an atomic structure. Now the goals are specified in terms of atoms, and the environment histories are specified in terms of Turing machines paired with an interaction history. How is the environment history evaluated in terms of atoms? This is the ontology identification problem.

Technical problem (Ontology Identification). :
- Given goals specified in some ontology and a world model, how can the ontology of the goals be identified in the world model? 
- What types of world models are amenable to ontology identification? For a discussion, see Soares (2015).

[...]

Agents built to solve the wrong problem—such as optimizing their observations—may well be capable of attaining superintelligence, but it is unlikely that those agents could be aligned with human interests (Bostrom 2014, chap. 12). A better understanding of naturalized induction and ontology identification is needed to fully specify the problem that intelligent agents would face when pursuing human goals while embedded within reality, and this increased understanding could be a crucial tool when it comes to designing highly reliable agents.",,"Nate Soares, Benya Fallenstein",nate@intelligence.org,false,true,0,,,https://intelligence.org/files/TechnicalAgenda.pdf,,2017-01-01,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
112,2022-07-15 12:39:26.808173+00,Recursive self-improvement,"This idea is part of The Learning-Theoretic AI Alignment Research Agenda. These ideas can be worked on in parallel.

The aim of this part of the agenda is formalize and analyze the concept of ""recursive self-improvement"" in learning-theoretic language.

Recursive self-improvement as a method of extremely rapid capability growth is an intriguing idea, however so far it has little rigorous support. Moreover, it far from clear that the first AGI will be recursively self-improving, even if the concept is sound. Therefore, I do not see it as high priority item on the agenda. Nevertheless, it is worth some attention both because of the capability angle and because of possible applications to decision-theory.

At present, I have only a few observations on how the subject might be approached:

- The dangers of self-modification can be naturally regarded as ""traps"", due to the irreversible nature of self-modification. Therefore, it seems appropriate to address them by the mechanisms of DRL. The way I expect it to cash out in practice is: (i) the AI will initially not self-modify directly but only by suggesting a self-modification and delegating its acceptance to the advisor (ii) a self-modification will only be approved if ""annotated"" by a natural language explanation of its safety and merit (iii) the explanation will be honest (non-manipulative) due to being sampled out of a the space of explanations that the advisor might have produced by emself.

- One way to view self-modification is as a game, where the players are all the possible modified versions of the agent. The state of environment also includes the modification state of the agent, so at each state there is one player that is in control (a ""switching controller"" stochastic game). Therefore, if we manage to prove theoretical guarantees about such games (for e.g. fuzzy reinforcement learning), these guarantees have implications for self-modification. It might be possible to assume the game is perfectly cooperative, since all modifications that change the utility function can be regarded as traps.

- The initial algorithm of the AI will already be computationally feasible (e.g. polynomial time) and will probably satisfy a regret bound close to the best possible. However, polynomial time is a only a qualitative property: indeed, we cannot be more precise without choosing a specific model of computation. Therefore, it might be that the capability gains from self-improvement should be regarded as, tailoring the algorithm to the particular model of computation (i.e. hardware). In other words, it involves the algorithm learning the hardware on which it is implemented (we could provide it with a formal specification of this hardware but this doesn't gain much as the agent would not know, initially, how to effectively utilize such a specification). Now, every improvement gained speeds up further improvements, but also there is some absolute upper bound: the best possible implementation. It is therefore interesting to understand whether there are asymptotic regimes for which the agent undergoes exponentially fast improvement during some local period of time, and if so, how realistic are these regimes.",,Vanessa Kosoy,,false,true,0,,,https://www.alignmentforum.org/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda-1,,2018-07-04,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
111,2022-07-15 12:37:59.935591+00,Taming daemons,"This idea is part of The Learning-Theoretic AI Alignment Research Agenda. These ideas can be worked on in parallel.

""Daemons"" are intelligent agents that are simulated, within some degree of fidelity, inside the AI's reasoning process and whose malign agendas can render the entire system unaligned even if the AI's ""main"" reasoning process is correctly designed to acquire human values in some sense. The aim of this part of the agenda is to formalize the problem and provide solutions in the form of theoretical guarantees.

I distinguish between two types of daemons: Cartesian and non-Cartesian. Cartesian daemons are those that are dangerous even if the Cartesian boundary between the AI and the rest of the universe is perfect. In other words, Cartesian daemons influence the word through the ""legitimate"" output channels given to the AI and while preserving the computational process inside the AI's hardware conformant to the software model assumed by the designers. On the other hand, non-Cartesian daemons rely on violations of the Cartesian boundary: they either exploit some physical side effects of the computation on the world or ""hack"" the AI from the inside in a way that breaks its software model (e.g. violate an abstraction layer in the software). Of course we might have a single (in some sense) subagent that utilizes both Cartesian and non-Cartesian attack mechanisms. In the following, we will refer to a ""Cartesian daemon"" in the sense of ""a Cartesian attack mechanism of some daemon"" and similarly for a ""non-Cartesian daemon"".

The taming of Cartesian daemons is guaranteed by the same regret bounds that guarantee the AI's alignment in general. For example, we can examine how it works in the case of the [acausal attack](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/) identified by Christiano. In this scenario, some malign superintelligence (let's call it Mu) somewhere in the multiverse simulates an environment very similar to that experienced by the AI we constructed on Earth. This makes the AI uncertain about whether it is on Earth or in Mu's realm, under some conditions assigning a higher probability to Mu. Mu then arranges the environment to diverge from Earth at some pivotal point s.t. the AI's expectation of this divergence leads it to perform malign actions before the divergence happens. Now, if the AI is implementing DRL, the uncertainty between Earth and Mu leads it to delegate to the advisor precisely at the moment this difference is important. Seeing the advisor display Earth-like rather than Mu-like behavior, it is then able to discard the Mu hypothesis.

[...]

Non-Cartesian daemons theoretically can be completely eliminated by placing the AI in a [homomorphic encryption box](https://www.alignmentforum.org/posts/2Wf3R4NZ77CLczLL2/cryptographic-boxes-for-unfriendly-ai). This relies mainly on two assumptions:
- The complexity theoretic conjecture underlying the cryptographic system is sound (specifically, the stronger conjecture that implies safety against quantum computing attacks as well).

- The physical universe is, in some sense, in the class BQP (as our current understanding of physics seems to indicate), or at least any stronger-than-BQP layer would be inaccessible from the box (which seems likely due to the usual separation of scales in physics).

[...]

We will then need some analysis which shows that, given the very small ""illegitimate"" bandwidth, the aligned AI will ""defeat"" the unaligned daemons with high probability. This requires some game-theoretic model incorporating physical assumptions the shape of which is currently unknown to me. Needless to say, we will also have to make sure the implementation fits the model, which might require, for example, extreme means to isolate the hardware from contaminants. There will be a tradeoff between making the model simpler and thus easier to verify, versus making the model more complex and thus easier to physically protect and possibly allowing for more efficient hardware.",,Vanessa Kosoy,,false,true,0,,,https://www.alignmentforum.org/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda-1,,2018-07-04,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
110,2022-07-15 12:32:04.453341+00,Value learning protocols,"This idea is part of The Learning-Theoretic AI Alignment Research Agenda. These ideas can be worked on in parallel.

The aim of this part in the agenda is developing learning setups that allow one agent (the AI) to learn the values of a different agent or group of agents (humans). This involves directly or indirectly tackling the issues of, what does it mean for an agent to have particular values if it is imperfectly rational and possibly vulnerable to manipulation or other forms of ""corruption"".
At present, I conceive of the following possible basic mechanisms for value learning:

- Formal communication: Information about the values is communicated to the agent in a form with pre-defined formal semantics. Examples of this are, communicating a full formal specification of the utility function or manually producing a reward signal. Other possibilities are, communicating partial information about the reward signal, or evaluating particular hypothetical situations.

- Informal communication: Information about the values is communicated to the agent using natural language or in other form whose semantics have to be learned somehow.

- Demonstration: The agent observes a human pursuing their values and deduces the values from the behavior.

- Reverse engineering: The agent somehow acquires a full formal specification of a human (e.g. an uploaded brain) and deduces the values from this specification. This is probably not a very realistic mechanism, but might still be useful for ""thought experiments"" to test possible definitions of imperfect rationality.

[...]

More generally, we can consider the following learning protocol that I call ""Learning By Teaching"" (LBT). We have our agent and two additional actors (in the simplest case, humans): an ""operator"" and an ""advisor"". The agent can, at each given moment, decide between 3 modes:
- Mode I: The operator and the advisor carry on without the AI's input. In this case, the advisor communicates something (the ""advice"") to the operator, and the operator takes actions that influence the external environment.

- Mode II: The AI acts instead of the advisor. Thus, the AI communicates advice to the operator, and the operator takes external actions.

- Mode III: The AI acts instead of the operator, taking external actions directly.

In all modes, all actors observe the percept produced by the environment.

",,Vanessa Kosoy,,false,true,0,,,https://www.alignmentforum.org/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda-1,,2018-07-04,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
109,2022-07-15 12:26:39.723014+00,Universal Reinforcement Learning,"This idea is part of The Learning-Theoretic AI Alignment Research Agenda. These ideas can be worked on in parallel.

The aim of this part in the agenda is deriving regret bounds or other performance guarantees for certain settings of reinforcement learning that are simultaneously strong enough and general enough to serve as a compelling definition / formalization of the concept of general intelligence. In particular, this involves solving the deficiencies of AIXI [...].

I believe that a key step towards this goal is solving the problem of ""irreflexivity"". That is, we need to define a form of reinforcement learning in which the agent achieves reasonable performance guarantees despite an environment which is as complex or more than the agent itself. My previous attempts to make progress towards that goal include [minimax forecasting](https://www.alignmentforum.org/posts/5bd75cc58225bf06703752b0/minimax-forecasting) and [dominant forecasters for incomplete models](https://arxiv.org/pdf/1705.04630.pdf). There, the aim was passive forecasting rather than reinforcement learning.

[...]

One hypothesis is, the main way humanity avoids traps is by happening to exist in a relatively favorable environment and knowing this fact, on some level. Specifically, it seems rather difficult for a single human or a small group to pursue a policy that will lead all of humanity into a trap (incidentally, this hypothesis doesn't reflect optimistically on our chances to survive AI risk), and also rather rare for many humans to coordinate on simultaneously exploring an unusual policy. Therefore, human history may be very roughly likened to episodic RL where each human life is an episode.

This mechanism should be formalized using the ideas of [quantilal control](https://www.alignmentforum.org/posts/5bd75cc58225bf0670375556/quantilal-control-for-finite-mdps). The baseline policy comes from the prior knowledge / advisor, and the allowed deviation (some variant of Renyi divergence) from the baseline policy is chosen according to the prior assumption about the rate of falling into a trap while following the baseline policy. This should lead to an appropriate regret bound.

[...]


[...] defining the correct universal prior and analyzing its properties is crucial to complete the theory. Given the hypotheses put forth in this section, this prior should be
- A fuzzy prior rather than a ""complete"" prior

- Possibly consist of a fuzzy version of finite, or otherwise restricted, MDPs (although Leike derives some regret bounds for general environments, at the cost of assuming sufficiently slowly dropping time discount and in particular ruling out geometric time discount). One way to think of it is, the finite MDPs are just an approximation of the infinite reality, however maybe we can also consider this a vindication of some sort of ultrafinitism.

- Reflect some ""favorability"" assumptions

- Have a hierarchical structure and consist of hierarchical models
",,Vanessa Kosoy,,false,true,0,,,https://www.alignmentforum.org/posts/5bd75cc58225bf0670375575/the-learning-theoretic-ai-alignment-research-agenda-1,,2018-07-04,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
108,2022-07-11 18:34:52.948829+00,Universal Alignment Test - An Anthropic Capture based pointer to human values,"An attempt to locate a robust and relatively specifiable pointer to human values using a modified version of Bostrom's Anthropic Capture, which relies on the AI's models about hypothetical simulators.

Currently being worked on by a team of four independent alignment researchers, has reviews  of ""seems interesting"" from the highest level people we've been able to reach. Not currently watertight due to a specific challenge around acausal bribery.

Contributors should have a firm grasp of acausal trade, simulations, the multiverse, red teaming, and big picture considerations around alignment such as those in the [Arbital alignment pages](https://arbital.greaterwrong.com/explore/ai_alignment/).

This is a good idea for developing the skills of red-teaming and iteration in a domain which is otherwise hard to find good feedback loops in, as you can find definite failure modes.

[WIP Google Doc](https://docs.google.com/document/d/1CMTS36MCbykYirTmC9Pdl2RBqLLPmrFU1sDcBNMvDCk/edit#heading=h.b1ygljkcjvpl) - Book a call with [plex](https://calendly.com/plex-eric-b) to share your thoughts or ask questions.",,plex,hello@plex.ventures,false,true,,,,https://docs.google.com/document/d/1CMTS36MCbykYirTmC9Pdl2RBqLLPmrFU1sDcBNMvDCk/edit#,,,0,$,,https://calendly.com/plex-eric-b,5cf69caa-c8f6-46ad-b37d-6215e620feaa,,true,false,,,false,,
107,2022-07-11 17:21:11.34508+00,The Windfall Clause: Sharing the benefits of advanced AI,"""The potential upsides of advanced AI are enormous, but there’s no guarantee they’ll be distributed optimally. In this talk, Cullen O’Keefe, a researcher at the Center for the Governance of AI, discusses one way we could work toward equitable distribution of AI’s benefits — the Windfall Clause, a commitment by AI firms to share a significant portion of their future profits — as well as the legal validity of such a policy and some of the challenges to implementing it.""

[EA Forum post](https://forum.effectivealtruism.org/posts/eCihFiTmg748Mnoac/cullen-o-keefe-the-windfall-clause-sharing-the-benefits-of), [Youtube video](https://www.youtube.com/watch?v=vFDL-NxY610&ab_channel=CentreforEffectiveAltruism0, [FHI Report overview](https://www.fhi.ox.ac.uk/windfallclause/), [Full report](https://www.fhi.ox.ac.uk/wp-content/uploads/Windfall-Clause-Report.pdf).",,"Cullen O’Keefe, Peter Cihon, Ben Garfinkel, Carrick Flynn, Jade Leung,Allan Dafoe",,false,true,500,,,https://www.fhi.ox.ac.uk/wp-content/uploads/Windfall-Clause-Report.pdf,,,0,$,,,f5d1e93f-5e17-4c2e-a2bb-7cd9a4a1ad64,,true,false,,,false,,
106,2022-07-11 13:55:04+00,"40,000 hours jam",Here we are,,,,false,false,243,,,,,,0,$,,,f5d1e93f-5e17-4c2e-a2bb-7cd9a4a1ad64,,true,false,,,false,,
104,2022-07-11 13:51:06+00,"40,000 hours","- The 80,000 hours to get mid-level career people involved. 
- Outreach, talent headhunting, career advice, write materials about how to switch careers or do the most good in a particular career, etc. 
- Which demographics make most sense? 
- Which career areas make most sense? 
- Would targeting EA's parents make sense? 
- etc.",,,,false,true,300,,,,,,0,$,,,f5d1e93f-5e17-4c2e-a2bb-7cd9a4a1ad64,,true,false,,,false,,
103,2022-07-10 18:10:02.607114+00,Avoid irreversibility of models,"To avoid lock-in, some want to train models to pursue easy-to-reverse states. One way is to increase optionality; however, current methods to do this might simultaneously increase power-seeking behavior. Perhaps in the future avoiding irreversibility generally and preventing lock-in can be separated from power-seeking. Right now it seems there are other more targeted approaches to avoiding lock-in, such as moral parliaments, philosophy research bot/value clarification, and cooperative AI.

",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
102,2022-07-10 18:09:16.230947+00,Proxy Gaming,"This is not clearly an area in its own right. Right now it looks like adversarial robustness, anomaly detection, and detecting emergent functionality, applied to sequential decision making problems. Perhaps in the future a distinct problem area will emerge.",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
101,2022-07-10 18:08:29.106088+00,Regulating Mesa-Optimizers and Intrasystem Goals,"As systems make objectives easier to optimize and break them down into new goals, subsystems are created that optimize these new intrasystem goals. But a common failure mode is that “intrasystem goals come first.” These goals can steer actions instead of the primary objective. Thus a system’s explicitly written objective is not necessarily the objective that the system operationally pursues, and this can result in misalignment.

Intrasystem goals occur when the goal of a training process (e.g., the loss function used for gradient descent, the exploration incentives of the sequential decision making agent, etc.) differs from the operational goal of the trained model it produces. This is known as mesa-optimization.

When multi-agent sequential decision-making is more feasible, we can give agents goals and delegate subgoals to agents. Since breaking down goals can distort them, this creates “intrasystem goals” and misalignment. Regulating these subagents that are optimizing their subgoal will be a research challenge. However, capabilities will need to be advanced further before this research area will be tractable.

An alternative way to study mesa optimizers is to study the general inductive biases of optimizers. While this could potentially be informative for understanding mesa-optimization, the neglectedness and tractability are low: this was previously the hottest area of theoretical ML for some years in the late 2010s, and see here for a discussion of tractability.

A related area that is more neglected and tractable is certified behavior. It is possible to have guarantees about model behavior given their weights, so it is not necessarily true that “all bets are off” when models are deployed.",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
100,2022-07-10 18:06:16.77773+00,Design cooperative AI,"Problem Description

In the future, AIs will interact with humans and other AIs. For these interactions to be successful, models will need to be more skilled at cooperating. This area is about reducing the prevalence and severity of cooperation failures. AI models and humans may be stuck in poor equilibria that are robustly difficult to escape; cooperative AI methods should improve the probability of escaping or avoiding poor equilibria. This problem also works towards making AI agents better at positive-sum games, of course subject to capabilities externalities constraints. As we describe this area, it does not include the typical directions in human-robot interaction, such as communication between humans and robots in standard tasks.

Motivation

First, worlds where multiple agents are aligned in different ways are highly plausible. There are strong incentives to have multiple decision-making agents; for example, jury theorems show collections of agents make better decisions than a single agent, and agents have incentives to retain some control and not automatically cede control to one single centralized agent.

In a world where we have AIs interacting with other agents, cooperative AI can be useful for not just having higher upside but also smaller downside. Cooperative AIs could help rein in misaligned agents or power-seeking AIs. For this protective measure to work, the power of the collective must be greater than the power of the power-seeking AI.

Let’s consider how easily a power-seeking AI could overpower the world. Of course, if AIs are better able to cooperate, they are more likely to counteract power-seeking AIs. Tracking and regulating the flow and concentration of GPUs can reduce the probability of a single AI becoming more powerful than the collective power of the rest. Even if one power-seeking agent is smarter than every other model, it does not imply that it has control over all other models. Usually, having higher cognitive ability does not let an agent overpower the collective (the highest IQ person does not rule the world). However, individual bad actors that are smarter than others can have outsized effects. In some special environments, such as environments with structured criticality, small differences could be magnified. Moreover, the world is becoming more long-tailed and more like “extremistan” in which there is tyranny of the top few (this is in contrast to mediocristan, where there is tyranny of the collective). Consequently, while there are factors that can give smarter models outsized advantages, the smartest model does not automatically overpower the collective power of cooperative AIs.

What Advanced Research Could Look Like

Researchers could create agents that, in arbitrary real-world environments, exhibit cooperative dispositions (e.g., help strangers, reciprocate help, have intrinsic interest in others achieving their goals, etc.). Researchers could create coordination systems or AI agent reputation systems. Cooperating AIs should also be more effective at coordinating to rein in power-seeking AI agents.

Importance, Neglectedness, Tractability

Importance: •••

Within systemic safety, cooperative AI is the most targeted towards the reduction of AI x-risk.

Neglectedness: •••

There are few researchers working in this area.

Tractability: •

It is currently especially difficult to perform meaningful research on multiagent sequential decision making.",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
99,2022-07-10 18:04:09.604188+00,ML for Improving Epistemics,"Problem Description

This area is about using machine learning to improve the epistemics and decision-making of political leaders. This area is tentative; if it turns out to have difficult-to-avoid capabilities externalities, then it would be a less fruitful area for improving safety.

Motivation

We care about improving decision-making among political leaders to reduce the chance of rash or possibly catastrophic decisions. These decision-making systems could be used in high-stakes situations where decision-makers do not have much foresight, where passions are inflamed, and decisions must be made extremely quickly and based on gut decisions. During these moments of peril, humans are liable to make egregious errors. Historically, the closest we have come to a global catastrophe has been in these situations, including the Cuban Missile Crisis. Work on these technologies could reduce the prevalence of perilous situations. Separately, this reduces the risks from persuasive AI. Moreover, it helps leaders more prudently wield the immense power that future technology will provide. As Carl Sagan said, “If we continue to accumulate only power and not wisdom, we will surely destroy ourselves.”

What Advanced Research Could Look Like

Systems could eventually become superhuman forecasters of geopolitical events. They could help brainstorming possible considerations that might be crucial to a leader’s decision. Finally, they could help identify inconsistencies in a leader’s thinking and help them produce a more sound judgment.

Importance, Neglectedness, Tractability

Importance: ••

Better epistemics could be useful for the development and deployment of AI systems, but it would not solve any fundamental problems in AI safety on its own.

Neglectedness: •••

Few care about superforecasting, let alone ML for forecasting.

Tractability: ••

This is an application of ML, but it is fairly outside the capabilities of current models.",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
98,2022-07-10 18:02:27.06733+00,Develop ML for Cyberdefense,"Problem Description

This area is about using machine learning to improve defensive security, such as by improving malicious program detectors. This area focuses on research avenues that are clearly defensive and not easily repurposed into offensive techniques, such as detectors and not automated pentesters.

Motivation

It will matter very little if AI systems are aligned if they can be hacked by humans or other AI systems and made to be misaligned (intentionally or unintentionally). There may also be situations where aligned AI is hijacked by a malicious actor who intentionally or accidentally contributes to x-risk. In addition, one of the fastest and most potent ways for a superintelligence to project its intelligence and influence the world is through cyberattacks, not through physical means.

Even if some of the components of ML systems are safe, they can become unsafe when traditional software vulnerabilities enable others to control their behavior. Moreover, traditional software vulnerabilities may lead to the proliferation of powerful advanced models, and this may be worse than proliferating nuclear weapons.

Cyberattacks could take down national infrastructure including power grids, and large-scale, reliable, and automated cyberattacks could engender political turbulence and great power conflicts. Great power conflicts incentivize countries to search the darkest corners of technology to develop devastating weapons. This increases the probability of weaponized AI, power-seeking AI, and AI facilitating the development of other unprecedented weapons, all of which are x-risks. Using ML to improve defense systems by decreasing incentives for cyberwarfare makes these futures less likely.

What Could Advanced Research Look Like?

AI-based security systems could be used for better intrusion detection, firewall design, malware detection, and so on.

Importance, Neglectedness, Tractability

Importance: ••

It’s important that powerful systems do not fall into the hand of extreme or reckless actors, but they may be able to develop those systems themselves regardless.

Neglectedness: •••

Security researchers are currently bottlenecked by compute power.

Tractability: •••

This is an application of ML, and doesn’t necessarily require new fundamental research.",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
97,2022-07-10 17:59:56.165341+00,Build model transparency tools for understanding AI systems,"Problem Description

AI systems are becoming more complex and opaque. This area is about gaining clarity about the inner workings of AI models and making models more understandable to humans.

Motivation

If humans lose the ability to meaningfully understand ML systems, they may no longer retain their sovereignty over model decisions.

Transparency tools could help unearth deception, mitigating risks from dishonest AI and treacherous turns.  This is because some speculate that deception could become inadvertently incentivized, and if models are capable planners, they may be skilled at obscuring their deception. Similarly, researchers could develop transparency tools to detect poisoned models, models with trojans, or models with other latent unexpected functionality. Moreover, transparency tools could help us better understand strong AI systems, which could help us more knowledgeably direct them and anticipate their failure modes.

What Advanced Research Could Look Like

Successful transparency tools would allow a human to predict how a model would behave in various situations without testing it. These tools should be able to be easily applied (ex ante and ex post emergence) to unearth deception, emergent capabilities, and failure modes.

To help make models more transparent, future work could try to provide clarity about the inner workings of models and understanding model decisions. Another line of valuable work is critiquing explainability methods and trying to show limitations of auditing methods. Measuring similarities and differences between internal representations is also an important step toward understanding models and their latent representations.

Importance, Neglectedness, Tractability

Importance: •••

If we could intuitively understand what models are doing, then they’d be far more controllable.

Neglectedness: •

This is highly funded by numerous stakeholders, and it has a large community. Deep nets are famous for being “black boxes,” and this limits their economic utility due to concerns about human oversight (such as in medical applications).

Tractability: •

This area has been struggling to find a solid line of attack throughout its existence. It has set goals for itself, and it has not met them (e.g., using transparency tools to find special functionality implanted by another human.)

",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
96,2022-07-10 17:56:33.334576+00,Trojan Horse Models,"Problem Description

AI systems can contain “trojan” hazards. Trojaned models behave typically in most situations, but when specific secret situations are met, they reliably misbehave. For example, an AI agent could behave normally, but when given a special secret instruction, it could execute a coherent and destructive sequence of actions. In short, this area is about identifying hidden functionality embedded in models that could precipitate a treacherous turn.

Motivation

One of the most dangerous sources of risk from advanced AI is sudden, unexpected changes in behavior. Similar to how people can hide their true intentions, a misaligned AI could bypass oversight mechanisms through deception. If we can uncover hidden behavior and predict treacherous turns before they happen, this will mitigate several failure modes.

What Researchers Are Doing Now

They are developing Trojan attacks and defenses. Most existing work uses CV datasets and models as a testbed, but recent work is beginning to explore Trojans for NLP models. A much smaller number of papers explores Trojans for RL. There is also related work on emergent behaviors in RL and emergent capabilities in large language models, which explores different aspects of hidden functionality.

Importance, Neglectedness, Tractability

Importance: •••

Treacherous turns from advanced AI systems are a significant source of x-risk. Starting work on this problem early is important, and Trojan research is one way to make initial progress.

Neglectedness: ••

The field of Trojans in deep learning is 5 years old, but there is still much left to be done. The field is not commonly associated with safety, so there is an opportunity to focus the field towards greater x-risk relevance and create a path for safety researchers to gain career capital.

Tractability: ••

Analyzing and detecting Trojans is an early field with much low-hanging fruit. It is a standard ML research problem with emerging benchmarks that can be iterated on. However, the problems of detecting and reverse-engineering Trojans are broad in scope and challenging.",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
95,2022-07-10 17:52:34.841239+00,Making models' uncertainty interpretable,"Problem Description

This area is about making model uncertainty more interpretable and calibrated by adding features such as confidence interval outputs, conditional probabilistic predictions specified with sentences, posterior calibration methods, and so on.

Motivation

If operators ignore system uncertainties since the uncertainties cannot be relied upon or interpreted, then this would be a contributing factor that makes the overall system that monitors and operates AIs more hazardous. To draw a comparison to chemical plants, improving uncertainty calibration could be similar to ensuring that chemical system dials are calibrated. If dials are uncalibrated, humans may ignore the dials and thereby ignore warning signs, which increases the probability of accidents and catastrophe.

Furthermore, since many questions in normative ethics have yet to be resolved, human value proxies should incorporate moral uncertainty. If AI human values proxies have appropriate uncertainty, there is a reduced risk in an human value optimizer maximizing towards ends of dubious value.

What Advanced Research Looks Like

Future models should be calibrated on inherently uncertain, chaotic, or computationally prohibitive questions that extend beyond existing human knowledge. Their uncertainty should be easily understood by humans. Moreover, given a lack of certainty in any one moral theory, AI models should accurately and interpretably represent this uncertainty in human value proxies.

Importance, Neglectedness, Tractability

Importance: ••

This is an important part of interpretability.

Neglectedness: •

Many people are working on it, maybe half an order of magnitude more than anomaly detection. Calibration in the face of adversaries is highly neglected, as are new forms of interpretable uncertainty: having models output confidence intervals, having models output structured probabilistic models (e.g., “event A will occur with 60% probability assuming event B also occurs, and with 25% probability if event B does not”).

Tractability: ••

There are shovel-ready tasks, and the community is making progress on this problem.",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
94,2022-07-10 17:51:20.872906+00,Anomaly Detection,"Problem Description

This area is about detecting potential novel hazards such as unknown unknowns, unexpected rare events, or emergent phenomena. Anomaly detection (also known as out-of-distribution detection) can allow models to flag salient anomalies for human review or execute a conservative fallback policy.

Motivation

This is an indispensable tool for detecting a wide range of hazards. For example:

- proxy gaming
- rogue ML systems that are already causing harm at smaller scales
- deceptive ML systems not easily detectable by humans
- Trojan horse models (discussed below)
- malicious users who may attempt to intentionally misalign a model, or align it to their own nefarious ends
- early signs of dangerous novel technologies
- AI tripwires could help uncover early misaligned systems before they can cause damage
- emergent behavior

This approach operates by virtue of being able to detect other hazards before they occur or before they can cause more damage. While in an ideal world the other hazards do not occur at all, in reality any serious attempt at safety must build in some mechanisms to detect failures to prevent hazards. Early detection is crucial to being able to successfully stop the hazard before it becomes impossible to do so.

In addition to helping with AI x-risk, anomaly detection can also be used to help detect novel engineered microorganisms that present biological x-risks, perhaps by having image and sequence anomaly detectors scan hospitals for novel pathogens (see this paper for an example of anomaly detection involving genomic sequences). Anomaly detection could also help detect Black Balls (“a technology that invariably or by default destroys the civilization that invents it”).

What Advanced Research Could Look Like

A successful anomaly detector could serve as an AI watchdog that could reliably detect and triage rogue AI threats. When the watchdog detects rogue AI agents, it should do so with substantial lead time. Anomaly detectors should also be able to straightforwardly create tripwires for AIs that are not yet considered safe. Furthermore, advanced anomaly detectors should be able to help detect black balls. Anomaly detectors should also be able to detect biological hazards by increasing detection lead time by having anomaly detectors continually scan hospitals for novel biological hazards.

Importance, Neglectedness, Tractability

Importance: •••

This cross-cuts many relevant problems.

Neglectedness: ••

After many years of community building, there are many researchers working in this area. The most cited paper in the area only has somewhat more than 1,000 citations, whereas there is an adversarial robustness paper with more than 10,000 citations.

Adversarial anomaly detection is more neglected. Anomaly detection for text-based models is also more neglected.

Tractability: ••

Detecting image anomalies is one of the hardest problems in image understanding, along with adversarial robustness. Progress is even slower than in adversarial robustness, though that may be because the field is smaller.

",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
93,2022-07-10 17:48:53.252789+00,Adversarial Robustness,"Problem Description

Adversarial examples demonstrate that optimizers can easily manipulate vulnerabilities in AI systems and cause them to make egregious mistakes. Adversarial vulnerabilities are long-standing weaknesses of AI models. While typical adversarial robustness is related to AI x-risk, future threat models will be broader than today’s adversarial threat models. Since we are concerned about being robust to optimizers that cause models to make mistakes generally, we should make minimal assumptions about the properties of the adversary and work to make models that are robust to many kinds of attacks.

Motivation

In the future, AI systems may pursue goals specified by other AI proxies. For example, an AI could encode a proxy for human values, and another AI system could be tasked with optimizing the score assigned by this proxy. If the human value proxy is not robust to optimizers, then its vulnerabilities could be exploited, so this gameable proxy may not be fully safe to optimize. By improving the reliability of learned human value proxies, optimizers and adversaries would have a harder time gaming these systems. If gaming becomes sufficiently difficult, the optimizer can be impelled to optimize the objective correctly. Separately, humans and systems will monitor for destructive behavior, and these monitoring systems need to be robust to adversaries.

We often study adversarial robustness in the continuous domain (vision) because gradient descent is powerful in that setting. In contrast, adversarial attacks for text-based models are weaker and often not gradient-based. We’d like to defend against the most powerful adversaries since as models get more intelligent, the attacks will get more powerful. Gradient attacks for vision systems are already extremely strong, so it gives us a more direct view of how to defend against powerful optimizers.

What Advanced Research Could Look Like

Ideally, an adversarially robust system would make reliable decisions given adversarially constructed inputs, and it would be robust to adversaries with large attack budgets using unexpected novel attacks. Furthermore, it should detect adversarial behavior and adversarially optimized inputs. A hypothetical human value function should be as adversarially robust as possible so that it becomes safer to optimize. A hypothetical human value function that is fully adversarially robust should be safe to optimize.

Importance, Neglectedness, Tractability

Importance: •••

High reliability is necessary for safe superintelligence. Work on this problem is a necessary component of high reliability.

Neglectedness: •

Numerous researchers are working in this area. However, aspects are neglected. Adversarial robustness for larger-scale systems is rarely studied due to academic researchers lacking the necessary compute. Likewise, long-term threat models are underexplored: attack specifications may not be known beforehand, and attack budgets could be large.

Tractability: ••

There are shovel-ready problems and the research is (slowly) progressing.",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
92,2022-07-10 17:42:56.402629+00,Automated Moral Philosophy Research (Value Clarification),"Problem Description

This area is about building AI systems that can perform moral philosophy research. This research area should utilize existing capabilities and avoid advancing general research, truth-finding, or contemplation capabilities.

Motivation

The future will sharpen and force us to confront unsolved ethical questions about our values and objectives. In recent decades, peoples’ values have evolved by confronting philosophical questions, including whether to infect volunteers for science, how to equitably distribute vaccines, the rights of people with different orientations, and so on. How are we to act if many humans spend most of their time chatting with compelling bots and not much time with humans, and how are we to balance pleasure and enfeeblement? Determining the right action is not strictly scientific in scope, and we will need philosophical analysis to help us correct structural faults in our proxies.

To address deficiencies in our moral systems, and to more rapidly and wisely address future moral quandaries that humanity will face, these research systems could help us reduce risks of value lock-in by improving our moral precedents earlier rather than later. If humanity does not (or cannot) take a “long reflection” to consider and refine its values after it develops strong AI, then the value systems lying around may be amplified and propagated into the future. Value clarification reduces risks from locked-in, deficient value systems. Additionally, value clarification can be understood as a way to reduce proxy misspecification, as it can allow values to be updated in light of new situations.

We will need to decide what values to pursue and how to pursue them. If we decide poorly, we may lock in or destroy what is of value. It is also possible that there is an ongoing moral catastrophe, which we would not want to replicate across the cosmos.

What Advanced Research Could Look Like

Good work in value clarification would be able to produce original insights in philosophy, such that models could make philosophical arguments or write seminal philosophy papers. Value clarification systems could also point out inconsistencies in existing ethical views, arguments, or systems.

Importance: •••

This would reduce many ontological or systematic misalignment errors.

Neglectedness: •••

No one is working on this.

Tractability: •

This is a challenging problem. (Possible intermediate steps are described in Unsolved Problems.)",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
91,2022-07-10 17:40:52.581753+00,Implementing Moral Decision-Making,"Problem Description

This area is about building models to understand ethical systems and steering models to behave ethically.

This research area includes a few strategies:

- Model intrinsic goods and normative factors, as these will be relevant even under extreme world changes. This is in contrast to task preferences; book summarization preferences are less fundamental human values, and their relevance is more fragile under extreme world changes.
- Given moral systems, get models to abide by them with an artificial conscience or other forms of endogenous self-regulation.
- Implement an automated moral parliament to have models act appropriately in the face of moral uncertainty.

A generalization of this area is machine ethics, which is about making models that act ethically; this is an alternative formulation of Alignment.

Motivation

This line of work helps create actionable ethical objectives for systems to pursue. If strong AIs are given objectives that are poorly specified, they could pursue undesirable actions and behave unethically. If these strong AIs are sufficiently powerful, these misspecifications could create an existential catastrophe. Consequently, work in this direction helps us avoid proxy misspecification as well as value lock-in.

If our foremost goal is reducing the probability of destroying or permanently curtailing the potential of humanity, then it seems to make most sense to focus on aligning AI to the most important and time-tested values, namely those considered in normative ethics.

Other potential motivations:

- Robustness is easier to achieve in a limited area than it is to achieve across a very wide range of tasks. If there’s one place we want to ensure robustness, it’s moral decision-making.
- An artificial conscience can block morally suspect actions in AI systems by having direct access to action choices.

What Advanced Research Could Look Like

High-functioning models should detect situations where the moral principles apply, assess how to apply the moral principles, evaluate the moral worth of candidate actions, select and carry out actions appropriate for the context, monitor the success or failure of the actions, and adjust responses accordingly.

Models could represent various purported intrinsic goods, including pleasure, autonomy, the exercise of reason, knowledge, friendship, love, and so on. Models should be able to distinguish between subtly different levels of these goods, and these value functions should not be vulnerable to optimizers. Models should be able to create pros and cons of actions with respect to each of these values, and brainstorm how changes to a given situation would increase or decrease the amount of a given intrinsic good. They should also be able to create superhuman forecasts of how an action can affect these values in the long-term (e.g., how studying can reduce wellbeing in the short-term but be useful for wellbeing in the long-term), though this kind of research must be wary of capabilities externalities. Models should also be able to represent more than just intrinsic goods, as they should also be able to represent constantly-updating legal systems and normative factors including special obligations and deontological constraints.

Another possible goal is to create an automated moral parliament, a framework for making ethical decisions under moral and empirical uncertainty. Agents could submit their decisions to the internal moral parliament, which would incorporate the ethical beliefs of multiple stakeholders in informing decisions about which actions should be taken. Using a moral parliament could reduce the probability that we are leaving out important normative factors by focusing on only one moral theory, and the inherent multifaceted, redundant, ensembling nature of a moral parliament would also contribute to making the model less gameable. If a component of the moral parliament is uncertain about a judgment, it could request help from human stakeholders. The moral parliament might also be able to act more quickly to restrain rogue agents than a human could and act in the fast-moving world that is likely to be induced by more capable AI. We don’t believe the moral parliament would solve all problems, and more philosophical and technical work will be needed to make it work, but it is a useful goal for the next few years.

Importance, Neglectedness, Tractability

Importance: •••

Models that make decisions with regard for their morality would be far less likely to cause catastrophes.

Neglectedness: •••

A handful of people are working on this.

Tractability: ••

So far, there has been continual progress.",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
90,2022-07-10 17:37:35.82952+00,Honest AI,"Problem Description

Honest AI involves creating models that only output what they hold to be true. It also involves determining what models hold to be true, perhaps by analyzing their internal representations.

Motivation

If it is within a model's capacity to be strategically deceptive (i.e. able to make statements that the model in some sense knows to be false in order to gain an advantage) then treacherous turn scenarios are more feasible. Models could deceive humans about their plans, and then execute them once humans are no longer able to course-correct. Plans for a treacherous turn could be brought to light by detecting dishonesty, or models could be made inherently honest, allowing operators to query them about their true plans.

Other motivation formulations:

- We would like to prevent models from producing deceptive information.
- If models can be made honest and only assert what they believe, then they can produce outputs that are more representative and give human monitors a more accurate impression of their beliefs.
- Honesty helps facilitate cooperation among AIs, so it enables possibilities in cooperative AI. Honesty also undercuts collusion.

What Advanced Research Could Look Like

Good techniques could be able to reliably detect when a model's representations are at odds with its outputs. Models could also be trained to avoid dishonesty and allow humans to correctly conclude that models are being honest with high levels of certainty.

Importance, Neglectedness, Tractability

Importance: •••

This could reduce many inherent hazards. If models were completely honest, deception would be far more difficult, thereby greatly reducing the probability of a whole class of failure modes.

Neglectedness: •••

This is a new area, though the idea of “faithful outputs” is a very easy sell to the ML community, so its neglectedness will probably decrease soon.

Tractability: ••

Research is in its early stages, but there is some initial (forthcoming) research that makes progress.",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
89,2022-07-10 17:35:05.765318+00,Incentivize models for power-aversiveness,"Problem Description:

This area is about incentivizing models to avoid gaining more power than is necessary.

Motivation:

Strategic AIs tasked with accomplishing goals would have instrumental incentives to accrue and maintain power, as power helps agents more easily achieve their goals. Likewise, some humans would have incentives to build and deploy systems that acquire power, because such systems would be more useful. If power-seeking models are misaligned, they could permanently disempower humanity.

If agents are given the power to single-handedly destroy humanity, a single system failure could result in an existential catastrophe. If power is instead distributed among multiple agents, failure could be decorrelated. This is more likely if agents are not constantly trying to overpower the others.

What Advanced Research Could Look Like

Models could evaluate the power of other agents in the world to accurately identify particular systems that were attaining more power than necessary. They could also be used to directly apply a penalty to models so that they are disincentivized from seeking power. Before agents pursue a task, other models could predict the types of power and amount of power they require. Lastly, models might be developed which are intrinsically averse to seeking power despite the instrumental incentive to seek power.

Importance, Neglectedness, Tractability

Importance: •••

This could reduce many inherent hazards. Models that do not accrue too much power would be easier to shut down and correct, and they could cause less damage.

Neglectedness: •••

Right now it’s just a handful of people working on this.

Tractability: ••

There is an abundance of low-hanging fruit on the technical front, since almost no work has been performed in this area. However, this area may be less tractable because it relies on  sequential decision making, which is not as developed as other areas of machine learning.
",,"Dan Hendrycks, Thomas Woodside",,false,true,0,,,https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5,,2022-06-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
80,2022-07-10 13:33:27+00,80k/outreach for ML researchers,"We are significantly bottlenecked by people doing AI safety research. What if we made an outreach org to get people who have experience as ML engineers into AI alignment? It could do outreach over various methods, such as having booths at conferences, giving lectures to ML grad students, targeted ads online, content marketing, etc. It could provide explanations of the rationale, one-on-one coaching to find a good research path and get into the industry, provide introductory materials, etc. ",,,,false,true,340,,,,,,,,,,f5d1e93f-5e17-4c2e-a2bb-7cd9a4a1ad64,,true,false,,,false,,
88,2022-07-10 10:43:51.516555+00,Create a prediction algo for Trojaned neural network properties,"Participate in the neural [Trojan competition](https://trojandetection.ai/).

**Trojan Analysis Track**: Given a dataset of Trojaned networks spanning multiple data sources, predict various properties of Trojaned networks on a test set with held-out labels. This track has two subtracks: (1) target label prediction, (2) trigger synthesis. For more information, see [here](https://trojandetection.ai/tracks.html#trojan-analysis).",,,,false,true,180,,,,,,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
87,2022-07-10 10:41:52.902378+00,Train a classifier of Trojaned vs. neutral neural networks,"Participate in the [neural Trojan competition](https://trojandetection.ai/).

Trojan Detection Track: Given a dataset of Trojaned and clean networks spanning multiple data sources, build a Trojan detector that classifies a test set of networks with held-out labels (Trojan, clean). For more information, see [here](https://trojandetection.ai/tracks.html#trojan-detection).",,,,false,true,220,,,,,,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
86,2022-07-10 09:02:30.152808+00,AI Trustworthiness Index,It is impossible to engage in a warranted trust relationship with an AI (or a human for that matter) without considering its trustworthiness. Generic trustworthiness standards are necessary and must be augmented over time by use case specific guidelines.,,,,false,true,,,,,,,,,,,aaba5ea5-733c-46ad-b9d4-dcf2699e8726,Signal,false,false,,,false,,
79,2022-07-07 11:19:44.439176+00,Convert a simple dense network into symbolic code,"If given a fully trained model, can you meaningfully reduce the dimensionality / make it more interpretable to different degrees by creating a compiler that compiles neural networks into a program?

1. Write out a program that takes the weights of a neural networks and outputs a program directly representing the network.
2. Implement a ""variable of fuzzy representation"" μ to allow for different compressions of the symbolic program based on the variable's representation of ""how precisely should the program represent the network"".

Examples of possible implementations at different μ are:
- Complete re-representation of the network through direct activation function + variable execution
- Continuous neural inputs into binary if/else
- Abstracting the layer representation

I recommend using a 3-layer, sub-50 neuron network for iteration and ability to execute the compiled program.",,,,false,true,120,,,,,,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
78,2022-07-07 11:05:30.83872+00,Conduct a review of implementations of empathetic AI systems,"[""The curious case of Pretty Good human inner/outer alignment""](https://www.lesswrong.com/posts/A5XTgYmGnqEDnJdzJ/the-curious-case-of-pretty-good-human-inner-outer-alignment) and [""Humans are very reliable agents""](https://www.lesswrong.com/posts/28zsuPaJpKAGSX4zq/humans-are-very-reliable-agents) describe humans and human values as surprisingly aligned. It would be very interesting to see how existing researchers and engineers have implemented empathy in AI systems from the Brain-like AGI perspective.

We are already working on this and will be hiring a part-timer to finish up our existing work. If you'd like to help, just write me at esben@apartresearch.com.",,,,false,true,260,,,,,,3000,$,https://apartresearch.com,,0ab224d1-75b6-44e5-b868-26018ca607fe,,false,false,,,false,,
77,2022-07-05 09:43:04.396774+00,This is a super dumb idea,But this is clearly dumber.,,,,false,false,,,,,,,,,,,0ab224d1-75b6-44e5-b868-26018ca607fe,Signal,false,false,,,false,,
10,2022-06-29 18:10:23.14376+00,Find inverse scaling laws in large models,"**Compete in the [Inverse Scaling Laws challenge](https://github.com/inverse-scaling/prize).**

As language models get larger, they seem to only get better.
Larger language models score better on benchmarks and unlock new capabilities like arithmetic [\[1\]](#ref1), few-shot learning [\[1\]](#ref1), and multi-step reasoning [\[2\]](#ref2).
However, language models are not without flaws, exhibiting many biases [\[3\]](#ref3) and producing plausible misinformation [\[4\]](#ref4).
The purpose of this contest is to find evidence for a stronger failure mode: tasks where language models get **worse** as they become better at language modeling (next word prediction).

We will award up to $250,000 in total prize money for task submissions, distributed as follows:
1. Up to 1 Grand Prize of $100,000.
2. Up to 5 Second Prizes of $20,000 each.
3. Up to 10 Third Prizes of $5,000 each.

Read much more about the challenge **[here](https://github.com/inverse-scaling/prize)**.",,Ian McKenzie & Ethan Perez,,false,true,140,,,https://github.com/inverse-scaling/prize,,2022-06-27,100000,$,https://github.com/inverse-scaling/prize,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Frontier"",""label"":""A project at the frontier of AI safety research""}",false,false,,,false,,
76,2022-06-29 11:13:07.837468+00,How do language models handle Black Swans?,"Most language models are trained on a large dataset (i.e. [the pile](https://pile.eleuther.ai/)). Because of the costs associated they are expensive to update. Figuring out how they handle an uncertain future (like War in Ukraine and other [Black Swans](https://en.wikipedia.org/wiki/The_Black_Swan:_The_Impact_of_the_Highly_Improbable)) could therefore inform how reliable they are. Concretely, the problem will look at how they ""predict"" an uncertain variable (like the prize of oil) _after_ their training period (versus after). ",,,,false,true,10,,,,,,0,$,,,e400f22c-6699-4eb9-8ab7-7db455da8227,"{""value"":""Student"",""label"":""Student""}",false,false,,,false,,
75,2022-06-25 17:27:59.896824+00,Map the AIS ecosystem using Obsidian,"Use the note-taking and publishing tool [Obsidian](https://obsidian.md) to create a structured network graph of all major organizations and players in AI safety.

![Network graph example](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftansybradshaw22.files.wordpress.com%2F2021%2F03%2F609e2-1eqvxspuupnyto-rvrpawva.png&f=1&nofb=1)",,,,false,true,16,,,,,,100,$AUD,https://www.aisafetysupport.org/,https://www.aisafetysupport.org/,f75c1062-c6d5-46b3-b473-3c4b7a9642e2,"{""value"":""Student"",""label"":""Student""}",false,false,,,false,,
74,2022-06-21 08:10:33.098879+00,Visualisation tools for broad peaks,"It’s pretty straightforward to get a mathematical criterion for broad peaks, at least in networks trained to zero loss (e.g. see Vivek Hebbar’s setup [here](https://www.lesswrong.com/posts/wPudaEemohdYPmsye/information-loss-greater-than-basin-flatness)). It would be useful to get some visualisation tools which can probe that criterion in real nets. For instance, what does the approximate nullspace on the data-indexed side of df/d\theta(\theta, x_i) look like?

This is the sort of thing which has a decent chance of immediately revealing new hypotheses which weren’t even in our space of considerations. Broadness of optima have come up a few times in our thought experiments and empirical investigations so far, and we suspect that there are some pretty deep links between modularity of solutions and their broadness. Better visualisation tools might help illuminate this link.

Questions you could ask:

- Can you find a way to turn the [aforementioned mathematical setup](https://www.lesswrong.com/posts/wPudaEemohdYPmsye/information-loss-greater-than-basin-flatness) into a visualisation tool for broad peaks?
- Which kinds of graphs or visualisation tools might demonstrate broadness in an intuitive way?
- Are there other ways you can think of to quantify and visualise broadness?

[Read more here](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#10__Visualisation_tools_for_broad_peaks).",,Wentworth et al.,,false,true,0,,,https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#10__Visualisation_tools_for_broad_peaks,,2022-06-16,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Mid"",""label"":""Mid career""}",false,false,,,false,,
73,2022-06-21 08:08:01.056529+00,Measuring modularity and information exchange in simple networks,"As we’ve [discussed before](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run), we think a good measure of modularity should be deeply linked to concepts of information exchange and processing, and finding a measure which captures these concepts might be a huge step forwards in this project. Although no such measure is currently in use to our knowledge, there are several that have been suggested in the literature which try and gauge how much different parts of the network interact with each other. Most of them work by finding a “maximally modular partition” and measuring its modularity, with the distinctive part of the algorithm being how the modularity of a particular partition is calculated. For instance:

- Some are derived from tools used to analyse simple unweighted undirected graphs, e.g. the Q-score
Some look at the weights, using e.g. the matrix norms of convolutional kernels. 
- Some look at derivatives with respect to node input and output, coactivation of neurons, or mutual information of neurons
- We’re also currently working on a candidate measure based on counterfactual mutual information, which we’ll be making a post about soon.

It would be valuable to compare these different measures against each other, and see if some are more successful at capturing intuitive notions of modularity than others. 

This isn’t just a theoretical issue either. Right now, it’s looking like e.g. the matrix norm and node derivative measures give very different answers, where one might tell you that a network exhibits statistically significant modularity, whereas the other says there isn’t any.

This suggests the following experiment: taking a very simple system (e.g. the retina task), training it until it finds a solution, and benchmarking and visualising all of these measures against each other on the learned solution. 

Some questions you could ask:

- Which modularity measures give rise to similar “maximally modular partitions”? Which ones give partitions that are more similar than others? ([this paper](https://arxiv.org/abs/cond-mat/0505245) suggests a method for comparing the similarity of two different partitions)
- For small networks, you could try visualising the learned solutions and the partitions. Do some partitions look obviously more modular than others?
- Do your results change if you apply them on a solution which hasn’t yet attained perfect performance?
- Try to construct networks that Goodhart a particular measure. How difficult is this? Do the results look like something that a typical training process might select for?

[Read more here](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#9__Measuring_modularity_and_information_exchange_in_simple_networks).",,Wentworth et al.,,false,true,0,,,https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#9__Measuring_modularity_and_information_exchange_in_simple_networks,,2022-06-16,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Mid"",""label"":""Mid career""}",false,false,,,false,,
72,2022-06-21 00:00:04.015853+00,Noisy input channels,"Organisms in the real world can’t really be overparameterized relative to the vast amount of incoming information they are bombarded with. To the extent that you could imagine defining a loss function for them, it’d be functionally impossible to reach perfect loss on it. As a consequence, we think such systems need ways to get rid of noise in the input so they can focus on the information that matters most.

In contrast, a lot of current deep learning takes place in an overparameterized regime where we train to zero loss, meaning we fit the noise in the input data. One could wonder whether constructing problems where fitting the noise is impossible would evolve networks better designed to throw away superfluous information. Since throwing information away also seems associated with sparsity and with it modularity, we are considering this as another hypothesis for why biological NNs seem so much more modular than artificial ones.

To test this, one could add random Gaussian noise to the input and label of a CNN MNIST setup like the one described in ([1](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#1__Investigate__randomly__modulary_varying_goals_in_modern_deep_learning_architectures_)). To be clear, this noise would be different every time an input is evaluated. Then, you could see whether the network evolves to filter this noise out, and if that leads to sparsity/modularity.

Questions you could ask:

- Is there a trade-off between performance and modularity as you add more noise? 
- If the network does evolve to filter noise out, can you see what mechanism it’s using to do this?
- Is this method more effective at producing modular solutions when it’s combined with other methods outlined in this document, such as ([1](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#1__Investigate__randomly__modulary_varying_goals_in_modern_deep_learning_architectures_)) and ([4](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#4__Investigate_the_effect_of_local___total_connection_costs_on_modularity_in_neural_networks))?",,Wentworth et al.,,false,true,0,,,https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#8__Noisy_input_channels,,2022-06-16,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Mid"",""label"":""Mid career""}",false,false,,,false,,
71,2022-06-20 23:57:57.56553+00,Noisy channels & strongly interacting parameters,"Mostly so far we’ve discussed the modularity characteristics of solutions found by the search process, but another interesting question you could ask is: how modular are neural networks at initialisation?

There are two possible extremes of scenarios we might get. One we could call the **“scarce channels”** scenario:

- The vast majority of information doesn’t propagate very far through the system; it gets wiped out by noise
- Node-values in nonadjacent chunks of the system are roughly independent
- High modularity scores, because if information is wiped out at a distance then most of the interactions must be local
- In order for any information to be passed at all, a strong training signal is needed to select that information

And the other we could call the **“scarce modules”** scenario:

- Most parts of the system interact strongly with the rest of the system
- Node-values in nonadjacent chunks of the system are rarely independent
- Low modularity scores, because no chunk of the system can be interpreted as a module
- In order for a part of the system to turn into a module at all, a strong training signal is needed to induce a Markov boundary around it

It would be informative to initialise neural networks with random values and test which of these two scenarios is closer to reality, for the vast majority of random parameter settings.

Questions you could ask:

- Is the modularity score dependent on which randomisation method you’re using?
- How large are the modules being found by your modular decomposition (if using the Q-score, or something similar)?
- Can you find a way of quantifying how far information propagates in the network?

[Read more here](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#7__Noisy_channels___strongly_interacting_parameters).",,Wentworth et al.,,false,true,0,,,https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#2__Measure_the_broadness_of_modular_and_non_modular_optima_in_deep_learning_architectures__,,2022-06-16,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Mid"",""label"":""Mid career""}",false,false,,,false,,
70,2022-06-20 23:55:45.488525+00,Check if modularity just happens more if you have more parameters,"As we’ve mentioned several times by now, real biology is very modular. Old neural networks with 10-100 parameters are usually not modular at all (unless particular strategies like MVG are used with the explicit goal of producing modularity). Modern networks with lots of parameters are [supposedly](https://arxiv.org/pdf/2103.03386.pdf) [somewhat modular](https://arxiv.org/pdf/2110.08058.pdf) sometimes.

So one hypothesis might be that modularity emerges when you scale up the number of parameters, for some reason. Maybe handling interactions of every parameter with every other parameter just becomes infeasible for most optimisers as parameter count increases, and the only way they can still find solutions is by partitioning the network into weakly interacting modules.

If this were true, adding more parameters to a network should make it more modular. To test this, you could e.g. take a simple image recognition architecture performing a very narrow image recognition task on MNIST, and calculate its modularity score, as described in ([1](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#1__Investigate__randomly__modulary_varying_goals_in_modern_deep_learning_architectures_)). Then, you could look at architectures with increasingly deeper layers, solving more complicated image recognition tasks, and calculate their modularity scores as well. For large models, already trained ones could be used. 

Questions you could ask:

- Is there a relationship between size and modularity metrics like Q-score?
- How many modules are found in the optimal decomposition when calculating the Q-score? Does this number also increase with larger networks?

[Read more here](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#6__Check_if_modularity_just_happens_more_if_you_have_more_parameters_).",,Wentworth et al.,,false,true,0,,,https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#2__Measure_the_broadness_of_modular_and_non_modular_optima_in_deep_learning_architectures__,,2022-06-16,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Mid"",""label"":""Mid career""}",false,false,,,false,,
69,2022-06-20 23:53:50.199921+00,"Implement a modularity / MVG experiment for serial, rather than parallel, subtasks","Our current MVG experimental setups involve the retina recognition task, as well as the CNN MNIST experiments proposed above. These all have a structure where you’d expect a modular solution to have two modules handling different subtasks in parallel, the outputs of which are needed to perform a small, final operation to get the output. 

It would be interesting to see what happens in a case where the subtasks are serial instead. For example, for a “retina” task like the one [in the Kashtan 2005 paper](https://www.pnas.org/doi/pdf/10.1073/pnas.0503610102), the loss function might require classifying patterns on the “left” retina of the input in order to decide what to do with the “right” retina of the input. Or for a CNN setup with two input images, the ask might be to recognise something in the first image which then determines what you should look for in the second image.

![Theoretical images](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ac920d0f8b8c5c426e03eb2e484f0e8faec0c994acc1b10e.png/w_1686)

Questions you could ask:

- Which kinds of modular tasks are more naturally described as serial rather than parallel?
- Do you get more modularity in the serial rather than parallel case? Is it harder to find solutions?
- In serial tasks, is the effect of MVG different if you are changing just the (chronologically) first task, or just the second?

[Read more about this](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#5__Implement_a_modularity___MVG_experiment_for_serial__rather_than_parallel__subtasks_).",,Wentworth et al.,,false,true,0,,,https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#2__Measure_the_broadness_of_modular_and_non_modular_optima_in_deep_learning_architectures__,,2022-06-16,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Mid"",""label"":""Mid career""}",false,false,,,false,,
68,2022-06-20 23:51:59.190846+00,Investigate the effect of local / total connection costs on modularity in neural networks,"Real biology is [highly modular](https://www.lesswrong.com/posts/JBFHzfPkXHB2XfDGj/evolution-of-modularity). One of the big differences between real biology and neurons in our ML models is that connecting things in the real world is potentially costly and difficult. Lots of papers seem to suggest that this plays a role in promoting modularity (we will probably have a post about this coming out over the next few weeks). If you introduce a penalty for having too many connections into your loss function, this tends to give you more modular solutions. 

But in real biology, the cost of forming new connections doesn’t only depend on the total number of connections, but also how physically distant the things you are connecting are. One way you could replicate that kind of connection cost on a computer might be to give each node in your network a 1D or 2D “position index”, and penalise connections between nodes depending on the L2 distance between their indices (e.g. see [this paper](https://hal.archives-ouvertes.fr/hal-01300699/document) for one example way of approaching this problem).

Questions you might ask:

- Do the results in the paper linked above replicate?
- As a baseline comparison, how does the performance / modularity of solutions change when you use the standard “total” connection cost, rather than a locally weighted one?
- Do the results depend on how you position the neurons in your space? If so, are there some positioning methods that lead to more modular solutions?
- How do these results change when you look at bigger networks?

[Read more here](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#4__Investigate_the_effect_of_local___total_connection_costs_on_modularity_in_neural_networks).",,Wentworth et al.,,false,true,0,,,https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#2__Measure_the_broadness_of_modular_and_non_modular_optima_in_deep_learning_architectures__,,2022-06-16,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Mid"",""label"":""Mid career""}",false,false,,,false,,
67,2022-06-20 23:50:04.998483+00,Attempt to replicate the modularly varying goals experiment with logic gates in the original Kashtan & Alon 2005 paper,"Read the original [Kashtan & Alon 2005 paper](https://www.pnas.org/doi/pdf/10.1073/pnas.0503610102) to replicate.

We’ve focused on the neural network retina recognition task in the paper so far, since it’s more directly relevant to ML. But since that result so far hasn’t really replicated the way the paper describes ([see our earlier post](https://www.lesswrong.com/posts/XKwKJCXgSKhSr9bZY/project-intro-selection-theorems-for-modularity#_Unsuccessfully__Replicating_the_original_MVG_paper)), it would be important for someone to check if the same is true of the logic gate experiment. 

Kashtan still has the original 2005 paper code, and seems happy to hand it out to anyone who asks. He also seems happy to provide advice on how to get it running and replicate the experiment. Please don’t hesitate to email him if you’re interested.",,Wentworth et al.,,false,true,0,,,https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#2__Measure_the_broadness_of_modular_and_non_modular_optima_in_deep_learning_architectures__,,2022-06-16,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Mid"",""label"":""Mid career""}",false,false,,,false,,
66,2022-06-20 23:45:15.901636+00,Measure the broadness of modular and non-modular optima in deep learning architectures,"We have some theories that predict modular solutions for tasks to be on average broader in the loss function landscape than non-modular solutions. One could test this by making a CNN and getting it to produce some zero training loss solutions to a task, some of which were more modular than others (see experiment ([1](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#1__Investigate__randomly__modulary_varying_goals_in_modern_deep_learning_architectures_)) for more detail on this, and ([4](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#4__Investigate_the_effect_of_local___total_connection_costs_on_modularity_in_neural_networks)) for another proposed method to produce modularity), then vary the parameters of these optima slightly to see how quickly the loss increases.

We’ve recently done experiments with simple networks and the retina problem, and this intuition seems to hold up. But more replications here would be very valuable, to examine the extent to which the hypothesis is borne out in different situations / task / network sizes.

Questions you might ask:

- Is there a relationship between broadness of optima and modularity score?
- Do different kinds of tasks, or different sizes of networks, generally result in broader optima?

![Descriptive image](https://lh3.googleusercontent.com/Robsjy5tbmxIqObUeubj5TVaYKyC-f8_lswwa4frveXwZpCA--FILNEDGqKyQDndxnQ2BNCkvFS69kyj0DjQs2mnj71MZ_zQVg-J0L8_IQ-vjAZMvtki4qltqORvkXTxZxYUOHg1PNWgsZ_m1Q)

[Read more here](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#2__Measure_the_broadness_of_modular_and_non_modular_optima_in_deep_learning_architectures__).",,Wentworth et al.,,false,true,0,,,https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#2__Measure_the_broadness_of_modular_and_non_modular_optima_in_deep_learning_architectures__,,2022-06-16,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Mid"",""label"":""Mid career""}",false,false,,,false,,
65,2022-06-20 23:41:48.649165+00,Investigate (randomly) modulary varying goals in modern deep learning architectures.,"In [a post](https://www.lesswrong.com/posts/XKwKJCXgSKhSr9bZY/project-intro-selection-theorems-for-modularity), we discussed the idea of [modularly varying goals](https://www.lesswrong.com/posts/JBFHzfPkXHB2XfDGj/evolution-of-modularity) in more detail. We conjectured that rapidly switching between a small set of different goals might fine-tune the network to deal with this specific switch, rather than providing selection pressure for modularity. One possible solution to this is RMVG (randomly-sampled modularly varying goals), where we switch between a large set of random goals, all still with the same subtasks, but have a large enough set that we never show the network the same task twice. 

[Read more here](https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run#1__Investigate__randomly__modulary_varying_goals_in_modern_deep_learning_architectures_).

Questions you might ask:
- Does this way of varying goals lead to more modular solutions than the fixed goal of `a × M1 + b × M2`?
- Can you think of any other modular goals which you could run a similar test for? Are there some types of modular goals which produce modularity more reliably?
- Does switching training methods (e.g. ADAM to SGD or GD) change anything about the average modularity score of solutions?",,Wentworth et al.,,false,true,0,,,https://www.lesswrong.com/posts/99WtcMpsRqZcrocCd/ten-experiments-in-modularity-which-we-d-like-you-to-run,,2022-06-16,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Mid"",""label"":""Mid career""}",false,false,,,false,,
53,2022-06-05 21:38:33.675124+00,Collate the instances where AI safety / catastrophic / existential risks are mentioned in policy documents in different countries,"1) Direct impact: help people working on AI safety policy have a sense of how safety issues are taken into account in different parts of the world (US, EU, China...). Several people working on this told me that this would be super helpful.
2) Indirect impact: help the researcher have a sense of the AI governance landscape and key governance documents; have a sense of what kind of """"wording"""" related to AGI safety is out there

This has significant value for people working in the field sooner rather later.",,Caroline Jeanmaire,jeanmaire.caroline@gmail.com,false,true,15,,,,,2022-05-25,0,$,,jeanmaire.caroline@gmail.com,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Signal"",""label"":""Hard project with a high failure rate""}",false,false,,,false,,
46,2022-06-05 20:56:21.498444+00,Procedurally evaluating factual accuracy,"Request for research on the design of precise procedures for evaluating how factually accurate pieces of text are. This stands out to me as an area that is potentially valuable for reducing risks from advanced AI, while not requiring detailed knowledge of ML.

The main motivation for this request is that it is a problem that arises very naturally when attempting to train [truthful LMs](https://www.lesswrong.com/posts/jWkqACmDes6SoAiyE/truthful-lms-as-a-warm-up-for-aligned-agi). The most straightforward way to optimize the factual accuracy of a language model is to have humans evaluate the factual accuracy of model outputs, and to then optimize those evaluations using techniques like reinforcement learning. The procedure needs to be unambiguous because label noise hurts both ML training and labeler monitoring (not to mention other benefits). Subject to this constraint, the main criterion for the procedure should be that it produces good outcomes in the given real-world setting.

My main reasons for thinking that this research could be important for reducing risks from advanced AI are:

* Direct benefits. The research could result in procedures that are directly used to train more beneficial AI. The sorts of risks this could help mitigate are discussed in more depth in [Truthful AI](https://www.lesswrong.com/posts/aBixCPqSnTsPsTJBQ/truthful-ai-developing-and-governing-ai-that-does-not-lie) and [Risks from AI persuasion](https://www.lesswrong.com/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion).
* Preparation for harder versions of the problem. I expect advanced AI systems to pose analogous versions of the same problem, but to require more advanced solutions, in which AI systems assist with human evaluation, for example. Solutions to the simpler problem can serve both as a starting point for more advanced solutions and as a baseline against which they can be compared, and can help build relevant capacity.
* Supporting ML work on truthful LMs. Currently, the responsibility for this research falls largely on ML researchers trying to improve factual accuracy, who are forced to choose some procedure for evaluating factual accuracy. Yet they may lack the expertise required to do this well.",,Jacob Hilton,jhilton@openai.com,false,true,0,,,https://www.lesswrong.com/posts/2zEeb36XL6HLnjDkj/procedurally-evaluating-factual-accuracy-a-request-for,,2022-03-30,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
45,2022-06-05 18:44:32.837865+00,Work on aligning recommender systems,"Ivan Vendrov and Jeremy Nixon made [a compelling case](https://forum.effectivealtruism.org/posts/xzjQvqDYahigHcwgQ/aligning-recommender-systems-as-cause-area) on why working on aligning existing recommended systems can lead to significant social benefits but also have positive flow-through effects on the broader problem of AGI alignment. Recommender systems are likely the largest datasets of real-word human decisions currently existing. Therefore, working on aligning them will require significantly more advanced models of human preferences values, such as metrics of extrapolated volition. It could also provide a large-scale real-world ground to test techniques of human-machine communication as interpretability and corrigibility.",,Riccardo Volpato,,false,true,0,,,https://www.lesswrong.com/posts/nqTkfrnE4CkbMtmHE/research-ideas-to-study-humans-with-ai-safety-in-mind#8___Work_on_aligning_recommender_systems,,2020-07-03,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
44,2022-06-05 18:42:27.481287+00,Understanding the risks and benefits of “better understanding humans”,"Some think that if powerful AI systems could understand us better, such as by doing more advanced sentiment recognition, there would be a significant risk that they may deceive and manipulate us better.

On the contrary, others argue that if powerful AI systems cannot understand certain human concepts well, such as emotions, it may be easier for misaligned behaviour to emerge. While an AI having deceiving intentions would be problematic for many reasons other than its ability to understand us, it seems interesting to better understand the risks, benefits, and the trade-offs of enabling AI systems to understand us better. It might be that these are no different than any other capability, or it might be that there are some interesting specificities. 

[Some also argued](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB/p/q2rCMHNXazALgQpGH#2_2__The_base_optimizer) that access to human modelling could be more likely to produce mesa-optimizers, learnt algorithms that have their own objectives. This argument hinges on the idea that since humans often act as optimizers, reasoning about humans would lead these algorithms to learn about optimization. A more in-depth evaluation of what reasoning about humans would involve could likely provide more evidence about the weight of this argument.",,Riccardo Volpato,,false,true,0,,,https://www.lesswrong.com/posts/nqTkfrnE4CkbMtmHE/research-ideas-to-study-humans-with-ai-safety-in-mind#7___Understanding_the_risks_and_benefits_of__better_understanding_humans_,,2020-07-03,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
43,2022-06-05 18:40:04.901302+00,Understanding value formation in human brains,"This is something that I am less familiar about, but let me put it out there for debate anyway. Since we want to build systems that are aligned and compatible with human values, would it not be helpful to better understand how humans form values in their brains? I do not think that we should copy how humans form values, as there could be better ways to do it, but knowing how we do it could be helpful, to say the least. There is [ongoing work](https://www.nature.com/articles/ncomms15808.pdf) in neuroscience to answer such questions.",,Riccardo Volpato,,false,true,0,,,https://www.lesswrong.com/posts/nqTkfrnE4CkbMtmHE/research-ideas-to-study-humans-with-ai-safety-in-mind#6___Understanding_value_formation_in_human_brains,,2020-07-03,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
42,2022-06-05 18:38:30.374226+00,Understanding better what “learning from preferences” mean,"When talking about value alignment, I heard a few times an argument that goes like this: “while I can see that the algorithm is learning from my preferences, how can I know that it has learnt my preferences”? This is a hard problem since latent preferences seem to be somewhat unknowable in full. While we certainly need some work on ensuring generalisation across distributions and avoiding unacceptable outcomes, it would also be useful to better understand what would make people think that their preferences have been learnt. This could also help with concerns like gaming preferences or deceitfully soliciting approval. 

",,Riccardo Volpato,,false,true,0,,,https://www.lesswrong.com/posts/nqTkfrnE4CkbMtmHE/research-ideas-to-study-humans-with-ai-safety-in-mind#5___Understanding_better_what__learning_from_preferences__mean,,2020-07-03,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
41,2022-06-05 18:36:36.90834+00,Unpacking interpretability,"Interpretability seems to be a key component of [numerous concrete solutions to inner alignment](https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai) problems. However, it also seems that improving our understanding of transparency and interpretability is an open problem.

This probably requires both formal contributions around defining robust definitions of interpretability as well as the human cognitive processes involved in understanding, explaining and interpreting things. 

I would not be happy if we ended up with some interpretability tools that we trust for some socially idiosyncratic reasons but are not de-facto safe. I would be curious to see some work that tries to decouple these ideas and help us get out of the trap of interpretability as an [ill-defined concept](https://arxiv.org/abs/1606.03490).",,Riccardo Volpato,,false,true,0,,,https://www.lesswrong.com/posts/nqTkfrnE4CkbMtmHE/research-ideas-to-study-humans-with-ai-safety-in-mind#4___Unpacking_interpretability,,2020-07-03,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
40,2022-06-05 18:33:37.267556+00,Unlocking richer feedback signals,"[Jan Leike et al.](https://arxiv.org/abs/1811.07871) asked whether feedback-based models (such as [Recursive Reward Modelling](https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84) or [Iterated Distillation and Amplification](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616)) can attain sufficient accuracy with an amount of data that we can produce or label within a realistic budget. 

Explicitly expressing approval for a given set of agent behaviours is time-consuming and often an experimental bottleneck. Among themselves, humans tend to use more sample efficient feedback methods, such as non-verbal communication. The most immediate way of addressing this question is to work on understanding preferences and values from natural language, which is being tackled but still unsolved. 

Going further, can we train agents from head nods and other micro-expressions of approval? There are already existing examples of such work coming out of [Social Signal Processing](http://www.dcs.gla.ac.uk/~vincia/papers/sspsurvey.pdf). We can extend this idea as far as training agents using brain-waves, which would take us to Brain-Computer Interfaces, although this direction seems relatively further away in time. Additionally, it makes sense to study this because systems could develop it on their own and we would want to have a familiarity with it if they do.",,Riccardo Volpato,,false,true,0,,,https://www.lesswrong.com/posts/nqTkfrnE4CkbMtmHE/research-ideas-to-study-humans-with-ai-safety-in-mind#3___Unlocking_richer_feedback_signals,,2020-07-03,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
39,2022-06-05 18:29:45.981239+00,Demonstrate where factored cognition and evaluation work well,"Factored cognition and evaluation refer to mechanisms to address open-ended cognitive tasks by breaking them down (or factoring) into many small and mostly independent tasks. Note that the possibly recursive nature of this definition makes it hard to reason about the behaviour of these mechanisms in the limit. 

Paul Christiano already made the [case for better understanding factored cognition end evaluation](https://www.lesswrong.com/posts/cpewqG3MjnKJpCr7E/ought-why-it-matters-and-ways-to-help#Factored_evaluation)  when describing what [Ought](https://ought.org/) is doing and why it matters. 

Factored cognition and evaluation are major components of numerous concrete proposals to solve outer alignment, including Paul’s ones. It, therefore, seems important to understand the extent to which factored cognition and evaluation work well for solving meaningful problems. 

Rohin Shah and Buck Shlegeris [mentioned](https://futureoflife.org/2020/04/15/an-overview-of-technical-ai-alignment-in-2018-and-2019-with-buck-shlegeris-and-rohin-shah/) that they would love to see more research in this direction for similar reasons and also because it seems plausible to Buck that “this is the kind of thing where a bunch of enthusiastic people could make progress on their own”. ",,Riccardo Volpato,,false,true,0,,,https://www.lesswrong.com/posts/nqTkfrnE4CkbMtmHE/research-ideas-to-study-humans-with-ai-safety-in-mind#2___Demonstrate_where_factored_cognition_and_evaluation_work_well,,2020-07-03,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
38,2022-06-05 17:45:33.692721+00,Understand how specific alignment techniques interact with actual humans,"Many concrete proposals of AI Alignment solutions, such as [AI Safety via Debate](https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1), [Recursive Reward Modelling](https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84) or [Iterated Distillation and Amplification](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616) involve human supervision. However, as Geoffrey Irving and Amanda Askell [argued](https://distill.pub/2019/safety-needs-social-scientists) we do not know what problems may emerge when these systems interact with real people in realistic situations. Irving and Askell suggested a [specific list of questions to work on](https://distill.pub/2019/safety-needs-social-scientists/#questions): the list is primarily aimed at the Debate technique but knowledge gained about how humans perform with one approach is likely to partially generalize to other approaches (I also recommend reading the [LessWrong comments](https://www.lesswrong.com/posts/xkRRRZ7Pdny7AQK5r/link-openai-on-why-we-need-social-scientists) to their paper).",,Riccardo Volpato,,false,true,0,,,https://www.lesswrong.com/posts/nqTkfrnE4CkbMtmHE/research-ideas-to-study-humans-with-ai-safety-in-mind,,2020-07-03,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
37,2022-06-05 17:39:09.451528+00,How do current language models relate to AGI? What are the limits of the current paradigm?,"Context: I’m interested in collaborating on projects about language models from NLP such as GPT-3 and T5.

[Related work](https://www.lesswrong.com/posts/k2SNji3jXaLGhBeYP/extrapolating-gpt-n-performance).

Details: I have some specific projects in mind that I will discuss with applicants. I’m also open to considering projects proposed by applicants in these general areas. Applicants should have some background in machine learning and be comfortable reading and understanding new papers in ML (e.g. Neurips or ICML papers). It’s helpful to have taken a course in ML, implemented ML models, or written ML papers or blogposts. However, no formal credential in ML is required. In addition, any of the following skills are helpful:

* Experience with contemporary NLP models: e.g. applying models, training them, and doing published research in NLP
* Research experience in any area of machine learning or a related field. Evidence for this is an academic paper or a blogpost
* Background in analytic philosophy, formal logic, or “Agent Foundations”. Evidence for this would be university courses, workshops, blog posts, research papers or reference letters",,Owain Evans,,false,true,0,,,https://www.lesswrong.com/posts/f69LK7CndhSNA7oPn/ai-safety-research-project-ideas,,2021-05-21,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
36,2022-06-05 17:37:23.738406+00,Aligning large language models with human preferences and other normative criteria,"Context: I’m interested in collaborating on projects about language models from NLP such as GPT-3 and T5.

Related work [1](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models), [2](https://arxiv.org/abs/2009.01325v2), [3](https://arxiv.org/abs/2002.09758)

Details: I have some specific projects in mind that I will discuss with applicants. I’m also open to considering projects proposed by applicants in these general areas. Applicants should have some background in machine learning and be comfortable reading and understanding new papers in ML (e.g. Neurips or ICML papers). It’s helpful to have taken a course in ML, implemented ML models, or written ML papers or blogposts. However, no formal credential in ML is required. In addition, any of the following skills are helpful:

* Experience with contemporary NLP models: e.g. applying models, training them, and doing published research in NLP
* Research experience in any area of machine learning or a related field. Evidence for this is an academic paper or a blogpost
* Background in analytic philosophy, formal logic, or “Agent Foundations”. Evidence for this would be university courses, workshops, blog posts, research papers or reference letters",,Owain Evans,,false,true,0,,,https://www.lesswrong.com/posts/f69LK7CndhSNA7oPn/ai-safety-research-project-ideas,,2021-05-21,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
34,2022-06-05 17:12:32.072576+00,"In what way could value learning be dangerous, and how could it be made safer? ","Context: [A previous result](https://arxiv.org/abs/1712.05812) demonstrated that one cannot deduce the preferences of an irrational agent, without learning “structural assumptions”. Programming these assumptions into an AI, however, involves giving that AI knowledge about humans and the world - knowledge that might increase its power faster than its alignment.

Details: What is the safest way of deducing human preferences? This project will use a mixture of philosophical analysis, situational analysis, and computer science examples to explicate what kind of information provides the best increase in alignment without excessive increase in the AI’s power. The issue of practical symbol grounding will be explored if there is enough time - practical symbol grounding gives the AI a lot of power over the world, if it knows what various symbols *mean*.

The outputs of this project will be one paper on value learning, and possibly one on symbol grounding, and some examples of agents learning and (mis)behaving in various circumstances.",,Stuart Russell,,false,true,0,,,https://arxiv.org/abs/1712.05812,,2017-12-15,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
33,2022-06-05 17:07:11.650454+00,"Learning human preferences: black-box, white-box, and structured white-box access","Context: [This post](https://www.lesswrong.com/posts/9rjW9rhyhJijHTM92/learning-human-preferences-black-box-white-box-and) by Stuart Armstrong gives some relevant context on detecting preferences in agents. [This post](https://www.lesswrong.com/posts/m2bwD87ctjJDXC3SZ/ultra-simplified-research-agenda) summarises a research agenda on synthesising human preferences, with links to the full version given in the text. 

Details: This project will mainly be programming based, though a literature review of relevant control systems ideas will also be carried out.

Previous results demonstrated that the preferences of an irrational agent cannot be deduced from its behaviour, unless one makes a certain number of “structural assumptions” (or “normative assumptions”). This project will test how many such assumptions are needed.

The basic idea is to create models of agents in grid-world situations, agents with preferences and biases, and train a classifier to deduce their preferences, given various collections of true assumptions about the agents. These examples will be analysed to see what kinds of assumptions are best for deducing agent preferences, and how many are needed.

The outputs of the project should be a computer science paper and some programmed example agents that others could build on.",,Stuart Armstrong,,false,true,0,,,https://www.lesswrong.com/posts/9rjW9rhyhJijHTM92/learning-human-preferences-black-box-white-box-and,,2020-08-24,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
32,2022-06-05 15:41:19.231687+00,Model splintering: How can you automate moving from one model to another?,"**Context**: [This post](https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1) by Stuart Armstrong gives an overview of model splintering: examples, arguments for its importance, a formal setting allowing us to talk about it, and some uses we can put this setting to. This post looks at how the formalisms of Cartesian frames and generalised models relate to each other. 

**Details**: Humans are capable of extending moral values to new situations, when their previous concepts no longer apply. This is analogous to the ability of a reinforcement learning agent to generalise it’s previous reward signal to new situations, when the reward signal is no longer available and the environment is out of distribution.

This project posits that that is not a mere analogy: that the human capacity for extending moral values (which includes analytic philosophy and thought experiments to un-encountered situations) is a skill which can be transposed into algorithms, and further automated to extend to environments shaped by powerful AIs, about which humans have no current intuitions.

The main initial task is to collect references from analytic philosophy, human value changes, and out-of-distribution behaviour in algorithms. The insights from these areas should then be combined in the model-splintering formalism, and new algorithms created in this formalism to generalise for advanced AIs.

The output of this research should be a few publications on new methods for safely extending AI reward and values to new areas, and maybe some sample code.",,Stuart Armstrong,,false,true,0,,,https://www.lesswrong.com/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1,,2020-08-27,0,$,,,622e1793-b6e1-40a2-862f-fd235d4e2564,,false,false,,,false,,
31,2022-05-24 23:27:34.607175+00,Reverse-engineer human social instincts,"The implication of the [brain-like AGI series'](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8) assumptions is that there are circuits for various “innate reactions” that underlie human social instincts, they are located somewhere in the [“Steering Subsystem”](https://www.lesswrong.com/posts/hE56gYi5d68uux9oM/intro-to-brain-like-agi-safety-3-two-subsystems-learning-and) part of the brain (roughly the hypothalamus and brainstem), and they are relatively simple input-output functions. The goal: figure out exactly what those input-output functions are, and how they lead (after within-lifetime learning) to our social and moral thoughts and behaviors.

From a reinforcement learning perspective, you can write it out as two questions:
1. How does the brain’s RL algorithm work?
2. What exactly is the reward function?

My impression is that neuroscientists have produced many thousands of papers on question A, and practically none directly addressing question B. But I think question B is much more important for AGI safety. And the social-instincts-related parts of the reward function, which are upstream of morality-related intuitions, are most important of all.",,Steven Byrnes,,false,true,,,,https://www.lesswrong.com/posts/tj8AC3vhTnBywdZoA/intro-to-brain-like-agi-safety-15-conclusion-open-problems-1,,2022-05-17,0,$,,,,,false,false,,,false,,
30,2022-05-24 23:24:09.593185+00,Identify errors in brain-like AGI safety ideas,"Review the literature and substantiate or dispute the claims made in the [brain-like AGI safety sequence](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8). 

In the text of those posts, you’ll see various suggestions and pointers as to why I believe the various neuroscience claims that I made. But a careful, well-researched analysis has yet to be written, as far as I’m aware. (Or if it has, send me a link! Nothing would make me happier than learning that I’m reinventing the wheel by saying things that are already well-established and widely-accepted.)",,Steven Byrnes,,false,true,,,,https://www.lesswrong.com/posts/tj8AC3vhTnBywdZoA/intro-to-brain-like-agi-safety-15-conclusion-open-problems-1,,2022-05-17,0,$,,,,,false,false,,,false,,
29,2022-05-24 23:14:57.702925+00,Massive scale AI safety media,"One of the biggest issues of contemporary AI safety is outreach and getting new entrants. By creating a massive growth media company focused on the high entertainment value of sci-fi stories about AI, we can supply a unique and large amount of new entrants into AI safety.

There are several directions to take with this. A couple of examples are:
1. Creating a lobby organization within the Hollywood movie scene or the Seattle / San Francisco game development scene to put more focus on AI as a major risk.
2. Become a major publishing company with the sole purpose of creating viral media that spreads the message while retaining profitability.

Read the one-pager [here](https://docs.google.com/document/d/1-91TpPbEvBCrJnjZcXy7ZwxaO9kRMZ2jdh9MzaubO4I/edit#).",,Apart Research,esben@apartresearch.com,false,true,,,,https://docs.google.com/document/d/1-91TpPbEvBCrJnjZcXy7ZwxaO9kRMZ2jdh9MzaubO4I/edit#,,2022-04-11,0,$,,https://apartresearch.com,,,false,false,,,false,,
28,2022-05-24 23:11:02.673507+00,Model certificates company,"Many companies are using increasingly complex black box models, which puts them at risk of violating GDPR. We provide a model alignment certificate based on advanced quantitative tests and human-in-the-loop evaluations. The goal is to have the certificate be required for public procurements in the EU. Read the one-pager [here](https://docs.google.com/document/d/1RAIpuUZhpWJrTHjdYY8Cw6DX3Z2FnW8hPSZUzsVyd78/edit#).",,Apart Research,esben@apartresearch.com,false,true,,,,https://docs.google.com/document/d/1RAIpuUZhpWJrTHjdYY8Cw6DX3Z2FnW8hPSZUzsVyd78/edit#,,2022-04-11,0,$,,https://apartresearch.com/join,,,false,false,,,false,,
27,2022-05-24 23:09:15.011608+00,Precise legal advice via chatbot,"Current legal advice is expensive and inefficient if you hire people and inaccurate if you do it yourself. We provide a legal advice NLP assistance algorithm that takes in a simple paragraph description of the situation and problem, queries its native knowledge graph, generates a precise solution statement and enables ease-of-use through easy digital tools to fulfill the solution statement.

From an AI safety perspective, it is a question of how we can properly develop AI that has to be incredibly precise. Read the accompanying one-pager [here](https://docs.google.com/document/d/1GIU4H3w_4Cr3NLhGCuDsT0JB7XgTusc0JeubVK0Cevs/edit#).",,Apart Research,esben@apartresearch.com,false,true,,,,https://docs.google.com/document/d/1GIU4H3w_4Cr3NLhGCuDsT0JB7XgTusc0JeubVK0Cevs/edit#,,2022-04-11,0,$,,https://apartresearch.com/join,,,false,false,,,false,,
26,2022-05-24 23:07:01.577927+00,Aligned Model Creation Platform," Interpretable models are needed in medicine, ML developers’ time is expensive and LLMs and CV can revolutionize many medicinal solutions. We provide a platform similar to [DataRobot](https://www.datarobot.com/) but focused on interpretable LLMs and CV in the field of medicine with an interpretability score that enables non-technical oversight of the created models. Read the accompanying one-pager [here](https://docs.google.com/document/d/1VQepd8foaGlEkTkMWV1_Klsh3f4ZsdMTv2l9kk1BTD4/edit).",,Apart Research,esben@apartresearch.com,false,true,,,,https://docs.google.com/document/d/1VQepd8foaGlEkTkMWV1_Klsh3f4ZsdMTv2l9kk1BTD4/edit,,2022-04-04,0,$,,https://apartresearch.com/join,,,false,false,,,false,,
25,2022-05-24 23:03:42.507856+00,Impact data science jobs,"Data Analytics is a rapidly growing job market with a huge demand and IT workers want to work in value-driven companies. We provide a job platform for impact-driven data scientists with expert company profiles and features for data science job hunts. In the long term, we use the vast network of companies and applicants to create an ethical data science freelance network. Read the accompanying one-pager [here](https://docs.google.com/document/d/1k0t_vaEFEKIG9EW-TaaoHzVSrq9EeFSrP6dAzNigQg4/edit#).",,Apart Research,esben@apartresearch.com,false,true,,,,https://docs.google.com/document/d/1k0t_vaEFEKIG9EW-TaaoHzVSrq9EeFSrP6dAzNigQg4/edit#,,2022-04-05,0,$,,https://apartresearch.com/join,,,false,false,,,false,,
24,2022-05-24 23:02:32.23978+00,Aligned credit assessment,"Credit assessments stand to gain immensely from the predictive powers of AI. However, it is currently held back by insufficiently interpretable algorithms. We are building a domain-expert focused platform for creating transparent and equitable credit-scores that promote fairness and accuracy for lenders. We intend on revolutionizing the California and EU-markets. Read the one-pager [here](https://docs.google.com/document/u/1/d/1lE1FoOAruoX_Enx0bsFkN51lNorqbaTp9eqHQ6t2XIw/edit?usp=drive_web&ouid=114311210723367872573).

From an AI safety perspective, this goes into the publicity and implementation arm of AI safety. It will enable us to enter into a very favorable position in the AI markets.

**Be aware** that it is a very hard market to enter and you need a lot of credibility and ethos to be hired by governmental institutions.",,Apart Research,esben@apartresearch.com,false,true,,,,https://docs.google.com/document/d/1lE1FoOAruoX_Enx0bsFkN51lNorqbaTp9eqHQ6t2XIw/edit#,,2022-04-06,0,$,,https://apartresearch.com/join,,,false,false,,,false,,
23,2022-05-24 22:59:04.731031+00,Chatbot development platform,"Today, chatbots are low-tech solutions made with rule-based interaction schemes and without adaptability. We offer a platform for chatbot development that builds upon itself by using rules, customer-generated data and customer-provided earlier conversational data.

Chatbots generally need to be aligned with the purposes of the business and so creating a company that gains profit while investigating how we can use the theories of the field of AI safety can be very valuable. Read the accompanying one-pager [here](https://docs.google.com/document/d/1fOOj7DsCVooXHcWSJVgszAXtazayexYcPJzcyVzmAAc/edit#).

**Disclaimer**: After a couple of interviews, we found that the potential competitors in this space might be utilizing these sorts of methods quite well.",,Apart Research,esben@apartresearch.com,false,true,,,,https://docs.google.com/document/d/1fOOj7DsCVooXHcWSJVgszAXtazayexYcPJzcyVzmAAc/edit#,,2022-04-13,0,$,,https://apartresearch.com/join,,,false,false,,,false,,
22,2022-05-23 23:09:36.392238+00,Review the current methods for learning from human values,"Literature review of the current state of ""learning from human values"", and where the most promising directions are.

There is an important history in the journey of methods to learn from human values (e.g. Inverse Reinforcement Learning). I currently think it takes too long to land on the most recent findings of the field - the best summary I found is basically a meme.

Success looks like a blog post summarizing the history of learning from human values, presenting key findings, limitations and scope for further work on each item in the history. Conclusion: Where is the scope for more work today?",,Jamie Bernardi,,false,true,,,,,,2022-03-23,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Student"",""label"":""Student""}",false,false,,,false,,
21,2022-05-23 23:06:16.577078+00,Testing ground for interpretability,"Program a simple environment in which we train neural agents to do simple tasks, but some agents are trained to be malicious. Set this up as a testing ground for network analysis that have to find the malicious agents. The networks should be small scale and the training algorithms rather simple.

It is a more practical and near term setup to check if we can solve small scale problems with the ideas we have. The malicious agents cannot be trivially found (like with gradient search over inputs that activate malicious neurons). Put the project on Github and make a blog post as a challenge.",,Stephan Wäldchen,,false,true,,,,,,2022-03-23,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Signal"",""label"":""Hard project with a high failure rate""}",false,false,,,false,,
20,2022-05-23 22:59:36.145828+00,Decompose complex tasks for IDA,"Decompose complex tasks for IDA. [Recursively Summarizing Books with Human Feedback (Wu et al. 2021)](https://arxiv.org/abs/2109.10862) used recursion to decompose the task of summarizing fiction novels in order to make this task tractable to iterated distillation and amplification (IDA). Think about some other complex tasks that it would be useful for AIs to accomplish and decompose them. [Ought's taxonomy of decomposition methods](https://ought.org/research/factored-cognition/taxonomy#recursion) might be useful here (recursion is just one approach).

IDA is a promising research direction in AI Safety. It depends on tasks being decomposed into simple subtasks though. So decomposing complex tasks could help prepare more problems to be solved by IDA. It could also help us learn more generally about how to decompose complex tasks so that we might eventually automate this process.

Success would look like a blog post breaking down the decomposition of some valuable complex tasks for AI to perform.",,Evan Murphy,,false,true,,,,,,2022-03-23,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Student"",""label"":""Student""}",false,false,,,false,,
19,2022-05-23 22:56:36.437854+00,Find domain dependence of InstructGPT's truthfulness,"Consider questions where the most common answer online is likely to be false (as in [TruthfulQA](https://arxiv.org/abs/2109.07958)). If you prompt GPT-3-Instruct (or similar) with questions and correct answers in one domain/topic, then ask it about another domain/topic, will it tend to give the correct answer or the popular answer? As you make the domains/topics more or less different, how does this vary?",,Sam Bowman,,false,true,,,,,,2022-03-23,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Student"",""label"":""Student""}",false,false,,,false,,
18,2022-05-23 22:53:27.337776+00,Create simple RL environments that illustrate open problems in alignment,"E.g. environments with a broad distribution of train time goals vs. unseen test-time goals to illustrate inner misalignment, or implementing an [“ELK”](https://ai-alignment.com/eliciting-latent-knowledge-f977478608fc)-style environment in code, complete with “sensors” that the AI has the ability to “tamper” with as an action.

Clear technical illustrations/benchmarks of open problems are a crucial tool for legitimizing research directions within the broader ML research community - it’s useful to be able to point to a specific benchmark and say “we should worry, nobody has a way to solve this and they’ve been trying for X years with Y pot of money as an incentive!”.

 If you do a good job of this, I will do my best to work with you to create [“Millenium Prizes”](https://en.wikipedia.org/wiki/Millennium_Prize_Problems) (of thousands to millions of dollars) for people who can achieve certain performance thresholds e.g. a prize to anyone who could propose an RL algorithm that solves an [inner misalignment](https://www.alignmentforum.org/tag/inner-alignment) benchmark. (if you provide the env/task implementation, I will work to provide the institutional housing, funding, and publicity for the prizes). (Note that of course more than one team can/should do this; I think this could easily accommodate 10 serious teams coding different illustrative alignment problems.)",,Yonadav Shavit,,false,true,,,,,,2022-03-23,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Signal"",""label"":""Hard project with a high failure rate""}",false,false,,,false,,
17,2022-05-23 22:50:52.833627+00,Argue why interpretability is useful for long-term AI alignment,"This would partially answer the question ""What relatively well-scoped research activities are particularly likely to be useful for longtermism-oriented AI alignment?"" from Holden Karnofsky's [Important, actionable research questions for the most important century](https://forum.effectivealtruism.org/posts/zGiD94SHwQ9MwPyfW/important-actionable-research-questions-for-the-most#Questions_about_AI_strategy__more_). Answering questions from this list could accelerate AI safety research by helping to allocate funding in the space. Create a report and interview relevant researchers in the field to get the best idea.

\[Spoiler\] Finished versions of this task: [Interpretability research for the most important century](https://www.alignmentforum.org/s/jef8ntrWuJ7SvZjCM/p/MygKP4iwdRL24eNsY) and [Analysis of 7 scenarios for interpretability's alignment-solving potential](https://www.alignmentforum.org/s/jef8ntrWuJ7SvZjCM/p/FrFZjkdRsmsbnQEm8).",,Holden Karnofsky,,false,true,,,,,,2022-03-14,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Student"",""label"":""Student""}",false,false,,,false,,
16,2022-05-23 22:46:00.981909+00,Make a steelman argument against AI safety,There hasn't been a lot of very solid arguments against AI Safety and it is a good exercise for entrants into the field of AI safety. A successful project results in research and drafting out the strongest arguments.,,,,false,true,,,,,,2022-03-10,0,$,,,0ab224d1-75b6-44e5-b868-26018ca607fe,"{""value"":""Signal"",""label"":""Hard project with a high failure rate""}",false,false,,,false,,
15,2022-05-23 22:39:41.739244+00,Quantify the price of alignment,"Measure the performance of interpretable models and black box models to put metrics on the usability differences between the two. Analyze performance, ease-of-use, robustness, interpretability, and any other variables that make sense to track to evaluate usefulness of models.

Experience with model comparisons and working with advanced code repositories with ML is useful for this project. ",,Esben Kran,,false,true,,,,https://docs.google.com/spreadsheets/d/1Ep8Dd52c00kxWvu75btazHEdPSA7jVJwrY51jKfPSBo/edit#gid=719912604,,2022-04-14,0,$,,https://docs.google.com/spreadsheets/d/1Ep8Dd52c00kxWvu75btazHEdPSA7jVJwrY51jKfPSBo/edit#gid=719912604,,,false,false,,,false,,